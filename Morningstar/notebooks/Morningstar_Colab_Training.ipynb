{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morningstar: Entraînement sur Google Colab (avec Téléchargement de Données)\n",
    "\n",
    "Ce notebook permet d'entraîner le modèle Morningstar en utilisant les ressources GPU de Google Colab.\n",
    "Il télécharge les données nécessaires directement depuis un échange spécifié.\n",
    "\n",
    "**Étapes:**\n",
    "1.  Configurer les paramètres de téléchargement des données (échange, paire, timeframe, dates).\n",
    "2.  Exécuter les cellules d'installation et de configuration.\n",
    "3.  Exécuter les cellules restantes pour télécharger et préparer les données, entraîner le modèle et sauvegarder/télécharger le résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paramètres de Téléchargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurez ces paramètres --- \n",
    "exchange_name = 'binance'  # Nom de l'échange (ex: 'binance', 'kraken', 'bybit')\n",
    "pair = 'BTC/USDT'         # Paire de trading à télécharger\n",
    "timeframe = '1h'          # Timeframe ('1m', '15m', '30m', '1h', '4h', '6h', '1d')\n",
    "start_date = '2022-01-01' # Date de début (YYYY-MM-DD)\n",
    "end_date = '2024-01-01'   # Date de fin (YYYY-MM-DD)\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation des Dépendances et Clonage du Projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "!pip install -q tensorflow pyarrow wandb pandas numpy ccxt ta asyncpraw\n", 
    "\n",
    "# Vérification GPU\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible: {gpu_devices}\")\n",
    "    # Configuration pour éviter les erreurs OOM sur certaines cartes\n",
    "    try:\n",
    "        for gpu in gpu_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth activé pour les GPUs.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Erreur lors de l'activation de memory growth: {e}\")\n",
    "else:\n",
    "    print(\"Aucun GPU détecté. L'entraînement se fera sur CPU (peut être très lent).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonage du dépôt GitHub (contient le code source du modèle et des utilitaires)\n",
    "!rm -rf eva001 # Supprimer le dossier s'il existe déjà pour éviter les conflits\n",
    "!git clone https://github.com/Cabrel10/eva001.git\n",
    "%cd eva001\n",
    "\n",
    "# Installation des dépendances spécifiques du projet\n",
    "!pip install -q ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Weights & Biases (W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à W&B (utilise la clé API fournie)\n",
    "!wandb login a1478933771f0389426436c0de1c39585a5a452c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Téléchargement et Préparation des Données\n",
    "\n",
    "Cette section télécharge les données OHLCV depuis l'échange, calcule les indicateurs techniques, et prépare les séquences et labels pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ajouter le répertoire racine du projet cloné au sys.path pour les imports\n",
    "project_root = '/content/eva001'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from Morningstar.configs.morningstar_config import MorningstarConfig\n",
    "from Morningstar.utils.data_manager import ExchangeDataManager\n",
    "# Importer les bibliothèques nécessaires pour les indicateurs\n",
    "import ta\n", 
    "from Morningstar.utils.custom_indicators import volume_anomality # Importer seulement la fonction nécessaire\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Téléchargement des données --- \n",
    "logger.info(f\"Téléchargement des données pour {pair} sur {exchange_name} ({timeframe}, {start_date} à {end_date})...\")\n",
    "manager = ExchangeDataManager(exchange_name)\n",
    "\n",
    "async def download_data():\n",
    "    await manager.load_markets_async() # Charger les marchés\n",
    "    df_raw = await manager.load_data(pair, timeframe, start_date, end_date)\n",
    "    await manager.close()\n",
    "    return df_raw\n",
    "\n",
    "# Exécuter la fonction asynchrone\n",
    "try:\n",
    "    # Utiliser nest_asyncio si déjà dans un event loop (cas de Colab/Jupyter)\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    df_ohlcv = asyncio.run(download_data())\n",
    "except RuntimeError as e:\n",
    "     # Si nest_asyncio n'est pas nécessaire ou cause une erreur, essayer sans\n",
    "     if 'cannot run loop while another loop is running' in str(e):\n",
    "         logger.warning(\"Event loop déjà actif, tentative sans nest_asyncio.\")\n",
    "         # Cette partie est plus complexe à gérer proprement dans un notebook.\n",
    "         # Pour simplifier, on pourrait lever une erreur ou essayer une autre approche.\n",
    "         # Alternative simple mais bloquante (si l'event loop existe déjà):\n",
    "         loop = asyncio.get_event_loop()\n",
    "         df_ohlcv = loop.run_until_complete(download_data())\n",
    "     else:\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors du téléchargement des données: {e}\")\n",
    "    raise\n",
    "\n",
    "if df_ohlcv.empty:\n",
    "    logger.error(\"Aucune donnée n'a été téléchargée. Vérifiez les paramètres (paire, dates, timeframe) et la connexion.\")\n",
    "    raise ValueError(\"Échec du téléchargement des données OHLCV.\")\n",
    "else:\n",
    "    logger.info(f\"Données OHLCV téléchargées avec succès. Shape: {df_ohlcv.shape}\")\n",
    "\n",
    "# --- Calcul des Indicateurs Techniques (Spécifiques au dataset cible) --- \n",
    "logger.info(\"Calcul des indicateurs techniques spécifiques...\")\n",
    "df_indicators = df_ohlcv.copy()\n",
    "try:\n",
    "    # RSI\n",
    "    df_indicators['rsi'] = ta.momentum.rsi(df_indicators['close'], window=14)\n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(df_indicators['close'])\n",
    "    df_indicators['macd'] = macd.macd()\n",
    "    df_indicators['macd_signal'] = macd.macd_signal()\n",
    "    df_indicators['macd_hist'] = macd.macd_diff()\n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(df_indicators['close'])\n",
    "    df_indicators['bb_upper'] = bollinger.bollinger_hband()\n",
    "    df_indicators['bb_middle'] = bollinger.bollinger_mavg()\n",
    "    df_indicators['bb_lower'] = bollinger.bollinger_lband()\n",
    "    # Volume MA\n",
    "    df_indicators['volume_ma'] = df_indicators['volume'].rolling(20).mean()\n",
    "    # Volume Anomaly (depuis custom_indicators)\n",
    "    df_indicators['volume_anomaly'] = volume_anomality(df_indicators)\n",
    "\n",
    "    logger.info(f\"Indicateurs techniques spécifiques ajoutés.\")\n",
    "    # Afficher les dernières lignes pour vérifier\n",
    "    # print(df_indicators.tail())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors du calcul des indicateurs: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Ajout des colonnes manquantes (Pair, Social/GitHub placeholders) --- \n",
    "logger.info(\"Ajout des colonnes Pair et placeholders Social/GitHub...\")\n",
    "df_indicators['pair'] = pair # Ajouter la colonne pair\n",
    "social_github_cols = ['commits', 'stars', 'forks', 'issues_opened', 'issues_closed']\n",
    "for col in social_github_cols:\n",
    "    df_indicators[col] = 0 # Ajouter les colonnes avec 0 par défaut\n",
    "\n",
    "# --- Préparation Finale (Nettoyage, Normalisation, Séquences, Labels) --- \n",
    "logger.info(\"Début de la préparation finale des données (features, NaN, scaling, séquences, labels, split)...\")\n",
    "config = MorningstarConfig()\n",
    "data = df_indicators.copy() # Utiliser le dataframe complété\n",
    "\n",
    "# 1. Sélection des features (Basé sur l'échantillon fourni)\n",
    "available_cols = data.columns.tolist()\n",
    "features = []\n",
    "def add_features(column_list):\n",
    "    added = [col for col in column_list if col in available_cols]\n",
    "    if len(added) != len(column_list):\n",
    "         missing = set(column_list) - set(available_cols)\n",
    "         logger.warning(f\"Colonnes de config manquantes après calcul indicateurs: {missing}\")\n",
    "    return added\n",
    "\n",
    "# Définir explicitement les features basées sur l'échantillon\n",
    "target_features = [\n",
    "    'open', 'high', 'low', 'close', 'volume', # Base OHLCV\n",
    "    'rsi', 'macd', 'macd_signal', 'macd_hist', # Indicateurs\n",
    "    'bb_upper', 'bb_middle', 'bb_lower', \n",
    "    'volume_ma', 'volume_anomaly',\n",
    "    'commits', 'stars', 'forks', 'issues_opened', 'issues_closed' # Placeholders Social/GitHub\n",
    "]\n",
    "\n",
    "# Vérifier si toutes les colonnes cibles existent après calculs/ajouts\n",
    "missing_cols = [col for col in target_features if col not in data.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Colonnes cibles manquantes dans le DataFrame préparé: {missing_cols}\")\n",
    "\n",
    "features = target_features # Utiliser cette liste pour la suite\n",
    "logger.info(f\"Colonnes finales sélectionnées pour le modèle ({len(features)}): {features}\")\n",
    "data = data[features].copy() # Sélectionner uniquement ces colonnes pour la suite\n",
    "\n",
    "# 2. Gestion des NaN (après calcul indicateurs et ajout placeholders)\n",
    "initial_rows = len(data)\n",
    "# Remplissage spécifique (ex: 0 pour indicateurs) ou ffill/bfill\n",
    "# Simple ffill pour commencer, puis dropna\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "data.fillna(method='bfill', inplace=True) # Pour les NaN au début\n",
    "data.dropna(inplace=True)\n",
    "rows_dropped = initial_rows - len(data)\n",
    "if rows_dropped > 0:\n",
    "    logger.warning(f\"{rows_dropped} lignes supprimées à cause de NaN restants après ffill/bfill.\")\n",
    "if data.empty:\n",
    "    raise ValueError(\"Le DataFrame est vide après la suppression des NaN.\")\n",
    "logger.info(f\"Données après gestion NaN: {len(data)} lignes\")\n",
    "\n",
    "# 3. Normalisation (StandardScaler par groupe)\n",
    "def scale_group(df, cols):\n",
    "    valid_cols = [col for col in cols if col in df.columns]\n",
    "    if valid_cols:\n",
    "         mean = df[valid_cols].mean()\n",
    "         std = df[valid_cols].std()\n",
    "         std[std == 0] = 1 # Éviter division par zéro\n",
    "         df[valid_cols] = (df[valid_cols] - mean) / std\n",
    "         logger.info(f\"Normalisation appliquée aux colonnes: {valid_cols}\")\n",
    "    return df\n",
    "\n",
    "data = scale_group(data, config.base_columns)\n",
    "data = scale_group(data, config.technical_columns)\n",
    "# data = scale_group(data, config.social_columns)\n",
    "# data = scale_group(data, config.correlation_columns)\n",
    "\n",
    "# 4. Création des séquences\n",
    "time_window = 60 # À définir ou récupérer depuis config si elle existe\n",
    "final_features = data.columns.tolist()\n",
    "logger.info(f\"Création des séquences (time_window={time_window}) avec {len(final_features)} features...\")\n",
    "\n",
    "total_sequences = len(data) - time_window\n",
    "if total_sequences <= 0:\n",
    "    raise ValueError(f\"Pas assez de données ({len(data)} lignes) pour créer des séquences avec une fenêtre de {time_window}.\")\n",
    "\n",
    "# Utiliser float32 pour économiser mémoire\n",
    "sequences = np.zeros((total_sequences, time_window, len(final_features)), dtype=np.float32)\n",
    "data_values = data[final_features].values.astype(np.float32)\n",
    "\n",
    "# Vectorisation pour créer les séquences (plus rapide que la boucle par chunk pour Colab)\n",
    "shape = (data_values.shape[0] - time_window + 1, time_window, data_values.shape[1])\n",
    "strides = (data_values.strides[0], data_values.strides[0], data_values.strides[1])\n",
    "sequences = np.lib.stride_tricks.as_strided(data_values, shape=shape, strides=strides)\n",
    "logger.info(f\"Séquences créées. Shape: {sequences.shape}\")\n",
    "\n",
    "# 5. Génération des labels (Triple Barrier - réplication de la logique)\n",
    "def _generate_labels(sequences: np.ndarray, data_df: pd.DataFrame, look_forward_steps: int = 5, pt_sl_ratio: float = 1.5) -> np.ndarray:\n",
    "    logger.info(f\"Génération des labels (Triple Barrier): look_forward={look_forward_steps}, pt_sl_ratio={pt_sl_ratio}\")\n",
    "    n_samples = sequences.shape[0]\n",
    "    close_col_index = final_features.index('close') # Index de la colonne 'close'\n",
    "    entry_prices = sequences[:, -1, close_col_index]\n",
    "    labels = np.zeros((n_samples, 3), dtype=np.float32)\n",
    "    labels[:, 2] = 1 # Initialiser à Hold\n",
    "\n",
    "    # Utiliser l'index du DataFrame original pour les prix futurs\n",
    "    future_prices = np.full((n_samples, look_forward_steps), np.nan, dtype=np.float32)\n",
    "    close_values = data_df['close'].values\n",
    "    # L'index de fin de la première séquence dans data_df est time_window - 1\n",
    "    # L'index de fin de la i-ème séquence est (time_window - 1) + i\n",
    "    for i in range(n_samples):\n",
    "        end_seq_idx = (time_window - 1) + i\n",
    "        start_future_idx = end_seq_idx + 1\n",
    "        end_future_idx = start_future_idx + look_forward_steps\n",
    "        if end_future_idx <= len(close_values):\n",
    "            future_prices[i] = close_values[start_future_idx:end_future_idx]\n",
    "\n",
    "    # Barrières simples (pourcentage fixe - à améliorer avec volatilité si besoin)\n",
    "    volatility = 0.01 # Approximation simple\n",
    "    upper_barrier = entry_prices * (1 + volatility * pt_sl_ratio)\n",
    "    lower_barrier = entry_prices * (1 - volatility)\n",
    "\n",
    "    for t in range(look_forward_steps):\n",
    "        current_future_prices = future_prices[:, t]\n",
    "        valid_indices = ~np.isnan(current_future_prices)\n",
    "        hold_indices = labels[:, 2] == 1\n",
    "        active_indices = valid_indices & hold_indices\n",
    "        if not np.any(active_indices): break # Arrêter si plus de labels à définir\n",
    "\n",
    "        # Indices actifs pour cette itération\n",
    "        current_active_indices = np.where(active_indices)[0]\n",
    "\n",
    "        # Vérifier barrière supérieure\n",
    "        hit_upper_mask = current_future_prices[current_active_indices] >= upper_barrier[current_active_indices]\n",
    "        hit_upper_indices = current_active_indices[hit_upper_mask]\n",
    "        if len(hit_upper_indices) > 0:\n",
    "             labels[hit_upper_indices] = [1, 0, 0] # Buy\n",
    "\n",
    "        # Mettre à jour les indices qui sont toujours 'Hold'\n",
    "        still_hold_mask = labels[current_active_indices, 2] == 1\n",
    "        active_indices_after_upper = current_active_indices[still_hold_mask]\n",
    "\n",
    "        # Vérifier barrière inférieure pour ceux qui sont toujours 'Hold'\n",
    "        hit_lower_mask = current_future_prices[active_indices_after_upper] <= lower_barrier[active_indices_after_upper]\n",
    "        hit_lower_indices = active_indices_after_upper[hit_lower_mask]\n",
    "        if len(hit_lower_indices) > 0:\n",
    "            labels[hit_lower_indices] = [0, 1, 0] # Sell\n",
    "\n",
    "    n_buy = np.sum(labels[:, 0])\n",
    "    n_sell = np.sum(labels[:, 1])\n",
    "    n_hold = np.sum(labels[:, 2])\n",
    "    logger.info(f\"Labels générés: Buy={int(n_buy)}, Sell={int(n_sell)}, Hold={int(n_hold)}\")\n",
    "    if n_buy == 0 and n_sell == 0 and n_hold == n_samples:\n",
    "         logger.warning(\"Aucun label Buy/Sell généré. Vérifiez la logique ou les données.\")\n",
    "    return labels\n",
    "\n",
    "labels = _generate_labels(sequences, data) # Utiliser le dataframe normalisé 'data'\n",
    "\n",
    "# 6. Split data\n",
    "def _split_data(sequences: np.ndarray, labels: np.ndarray, test_size=0.2):\n",
    "    split_idx = int(len(sequences) * (1 - test_size))\n",
    "    train_seq, test_seq = sequences[:split_idx], sequences[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "    logger.info(f\"Split data: Train ({train_seq.shape}, {train_labels.shape}), Test ({test_seq.shape}, {test_labels.shape})\")\n",
    "    return (train_seq, train_labels), (test_seq, test_labels)\n",
    "\n",
    "(train_seq, train_labels), (val_seq, val_labels) = _split_data(sequences, labels)\n",
    "\n",
    "# Créer les tuples finaux pour l'entraînement\n",
    "train_data = (train_seq, train_labels)\n",
    "val_data = (val_seq, val_labels)\n",
    "logger.info(\"Préparation des données terminée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialisation de l'Expérience W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialisation de l'expérience W&B pour le suivi\n",
    "try:\n",
    "    # Utiliser les paramètres définis plus haut dans la config W&B\n",
    "    wandb_config = {\n",
    "        'exchange': exchange_name,\n",
    "        'pair': pair,\n",
    "        'timeframe': timeframe,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'batch_size': config.batch_size,\n",
    "        'epochs': config.epochs,\n",
    "        'time_window': time_window,\n",
    "        'cnn_filters': config.cnn_filters,\n",
    "        'lstm_units': config.lstm_units,\n",
    "        'dense_units': config.dense_units\n",
    "    }\n",
    "    wandb.init(project=\"morningstar-colab-training-dl\", entity=\"cabrelkaka-morningstar\", config=wandb_config)\n",
    "    logger.info(\"Wandb initialisé avec succès.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'initialisation de Wandb: {e}\")\n",
    "    wandb_active = False\n",
    "else:\n",
    "    wandb_active = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chargement et Compilation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morningstar.model.architecture.morningstar_model import MorningstarTradingModel\n",
    "\n",
    "# Déterminer input_shape et num_classes à partir des données préparées\n",
    "input_shape = train_data[0].shape[1:] # (time_window, num_features)\n",
    "num_classes = train_data[1].shape[1] # Nombre de classes (Buy, Sell, Hold -> 3)\n",
    "\n",
    "logger.info(f\"Initialisation du modèle avec input_shape={input_shape}, num_classes={num_classes}\")\n",
    "\n",
    "# Instancier le modèle\n",
    "model_instance = MorningstarTradingModel(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    cnn_filters=config.cnn_filters,\n",
    "    lstm_units=config.lstm_units,\n",
    "    dense_units=config.dense_units\n",
    ")\n",
    "\n",
    "# Compiler le modèle\n",
    "model_instance.compile_model(learning_rate=config.learning_rate)\n",
    "logger.info(\"Modèle compilé.\")\n",
    "model_instance.model.summary() # Afficher le résumé du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configuration des Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Définition des callbacks pour l'entraînement\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    EarlyStopping(patience=15, monitor='val_loss', restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "if wandb_active:\n",
    "    try:\n",
    "        from wandb.keras import WandbCallback\n",
    "        callbacks.append(WandbCallback(save_model=False)) # Ne pas sauvegarder le modèle via W&B, on le fait déjà\n",
    "        logger.info(\"Callback Wandb ajouté.\")\n",
    "    except ImportError:\n",
    "        logger.warning(\"Impossible d'importer WandbCallback. Le suivi W&B sera limité.\")\n",
    "\n",
    "logger.info(f\"Callbacks configurés: {callbacks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entraînement du Modèle\n",
    "\n",
    "L'entraînement peut prendre du temps en fonction de la taille des données et du nombre d'époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Début de l'entraînement...\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy() if len(gpu_devices) > 1 else tf.distribute.get_strategy()\n",
    "logger.info(f\"Utilisation de la stratégie de distribution: {strategy.__class__.__name__}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    # Re-créer et compiler le modèle dans le scope est recommandé pour MirroredStrategy\n",
    "    model_instance_scoped = MorningstarTradingModel(\n",
    "        input_shape=input_shape,\n",
    "        num_classes=num_classes,\n",
    "        cnn_filters=config.cnn_filters,\n",
    "        lstm_units=config.lstm_units,\n",
    "        dense_units=config.dense_units\n",
    "    )\n",
    "    model_instance_scoped.compile_model(learning_rate=config.learning_rate)\n",
    "    logger.info(\"Modèle recréé et compilé dans le scope de la stratégie.\")\n",
    "\n",
    "    history = model_instance_scoped.model.fit(\n",
    "        x=train_data[0], \n",
    "        y=train_data[1], \n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        validation_data=val_data, \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "logger.info(\"Entraînement terminé.\")\n",
    "\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "logger.info(f\"Perte finale (entraînement): {final_train_loss:.4f}\")\n",
    "logger.info(f\"Perte finale (validation): {final_val_loss:.4f}\")\n",
    "\n",
    "if wandb_active:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde sur Google Drive (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    drive_path = '/content/drive/MyDrive/Morningstar_Models/' # Chemin sur votre Drive\n",
    "    os.makedirs(drive_path, exist_ok=True) \n",
    "    model_save_path = os.path.join(drive_path, f'best_model_{pair.replace(\"/\", \"_\")}_{timeframe}_{end_date}.h5')\n",
    "    \n",
    "    if os.path.exists('best_model.h5'):\n",
    "        !cp best_model.h5 \"{model_save_path}\"\n",
    "        logger.info(f\"Modèle sauvegardé sur Google Drive: {model_save_path}\")\n",
    "    else:\n",
    "        logger.warning(\"Le fichier 'best_model.h5' n'a pas été trouvé. Sauvegarde sur Drive annulée.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de la sauvegarde sur Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Téléchargement du Modèle Entraîné\n",
    "\n",
    "Exécutez la cellule suivante pour télécharger le fichier `best_model.h5` sur votre machine locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "if os.path.exists('best_model.h5'):\n",
    "    print(\"Téléchargement de best_model.h5...\")\n",
    "    files.download('best_model.h5')\n",
    "else:\n",
    "    logger.error(\"Le fichier 'best_model.h5' n'a pas été trouvé pour le téléchargement.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
