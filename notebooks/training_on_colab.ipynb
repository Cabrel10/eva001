{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425b5c7b",
   "metadata": {},
   "source": [
    "# Morningstar Pro - Entraînement avancé sur Colab\n",
    "\n",
    "## Système complet de trading algorithmique avec données sociales\n",
    "\n",
    "Ce notebook permet :\n",
    "- De télécharger les données de marché (OHLCV) depuis un exchange crypto\n",
    "- D'ajouter des indicateurs techniques avancés\n",
    "- D'intégrer des données sociales (GitHub et Reddit) si les APIs sont configurées\n",
    "- D'entraîner un modèle de deep learning pour le trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423691ce",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances système et Python\n",
    "# Utilisation de versions spécifiques pour TF, NumPy, Pandas\n",
    "!pip install -q tensorflow==2.12.0 pandas==1.5.3 numpy==1.23.5\n",
    "!pip install -q ccxt==4.1.91 ta pyarrow scikit-learn asyncpraw tweepy aiohttp PyGithub praw nest_asyncio jedi\n",
    "\n",
    "# --- IMPORTANT : Redémarrez MANUELLEMENT l'environnement d'exécution après cette cellule --- #\n",
    "# (Menu Exécution > Redémarrer l'environnement d'exécution)\n",
    "print(\"Dépendances installées. Veuillez redémarrer l'environnement d'exécution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123698fd",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonage du dépôt et ajout du chemin Morningstar\n",
    "# Assurez-vous que le kernel a été redémarré avant cette cellule\n",
    "import os\n",
    "if not os.path.exists('/content/eva001'):\n",
    "    !git clone https://github.com/Cabrel10/eva001.git\n",
    "else:\n",
    "    print(\"Dépôt déjà cloné.\")\n",
    "import sys\n",
    "if '/content/eva001' not in sys.path:\n",
    "    sys.path.insert(0, '/content/eva001')\n",
    "print(\"Chemin ajouté.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4bad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection interactive des paires et des dates\n",
    "import datetime\n",
    "default_pairs = 'BTC/USDT,ETH/USDT,BNB/USDT,SOL/USDT'\n",
    "pairs_input = input(f\"Entrez les paires séparées par une virgule (exemple: {default_pairs}): \") or default_pairs\n",
    "pairs = [p.strip().upper() for p in pairs_input.split(',')]\n",
    "start_date = input(\"Date de début (YYYY-MM-DD, défaut 2023-01-01): \") or '2023-01-01'\n",
    "end_date = input(\"Date de fin (YYYY-MM-DD, défaut aujourd'hui): \") or str(datetime.date.today())\n",
    "print(f\"Paires sélectionnées: {pairs}\")\n",
    "print(f\"Période: {start_date} à {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des APIs sociales (Optionnel)\n",
    "try:\n",
    "    from github import Github\n",
    "    import praw\n",
    "except ImportError:\n",
    "    print(\"Bibliothèques GitHub/Reddit non trouvées. Installation...\")\n",
    "    !pip install -q PyGithub praw\n",
    "    from github import Github\n",
    "    import praw\n",
    "\n",
    "# Config GitHub (optionnel)\n",
    "github_token = input(\"Entrez votre token GitHub (laissez vide pour ignorer): \") or None\n",
    "gh = None\n",
    "if github_token:\n",
    "    try:\n",
    "        gh = Github(github_token)\n",
    "        gh.get_user().login # Teste la connexion\n",
    "        print(\"GitHub API configurée.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur configuration GitHub: {e}. Les données GitHub ne seront pas collectées.\")\n",
    "        gh = None\n",
    "else:\n",
    "    print(\"Token GitHub non fourni. Les données GitHub ne seront pas collectées.\")\n",
    "\n",
    "# Config Reddit (optionnel)\n",
    "reddit_client_id = input(\"Reddit client_id (laissez vide pour ignorer): \") or None\n",
    "reddit_client_secret = input(\"Reddit client_secret (laissez vide pour ignorer): \") or None\n",
    "reddit = None\n",
    "if reddit_client_id and reddit_client_secret:\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=reddit_client_id,\n",
    "            client_secret=reddit_client_secret,\n",
    "            user_agent=\"Morningstar Data Collector by Cline\",\n",
    "            read_only=True # Mode lecture seule important\n",
    "        )\n",
    "        # Test simple pour vérifier la connexion\n",
    "        print(f\"Test de connexion Reddit pour l'utilisateur: {reddit.user.me()}\")\n",
    "        print(\"Reddit API configurée (mode lecture seule).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur configuration ou test Reddit: {e}. Les données Reddit ne seront pas collectées.\")\n",
    "        reddit = None\n",
    "else:\n",
    "    print(\"Credentials Reddit non fournis. Les données Reddit ne seront pas collectées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cd9bb",
   "metadata": {},
   "source": [
    "## 3. Pipeline de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96db1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour données sociales\n",
    "def get_github_stats(repo_name):\n",
    "    if not gh:\n",
    "        return 0, 0, 0, 0, 0 # Retourner 0 si non configuré\n",
    "    try:\n",
    "        repo = gh.get_repo(repo_name)\n",
    "        # Utiliser des méthodes qui ne requièrent pas d'itération complète si possible\n",
    "        commits_count = repo.get_commits().totalCount # Peut être lourd\n",
    "        stars_count = repo.stargazers_count\n",
    "        forks_count = repo.forks_count\n",
    "        open_issues_count = repo.get_issues(state='open').totalCount\n",
    "        closed_issues_count = repo.get_issues(state='closed').totalCount\n",
    "        return commits_count, stars_count, forks_count, open_issues_count, closed_issues_count\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur GitHub pour {repo_name}: {e}\")\n",
    "        return 0, 0, 0, 0, 0 # Retourner 0 en cas d'erreur\n",
    "\n",
    "def get_reddit_sentiment(subreddit, pair):\n",
    "    if not reddit:\n",
    "        return 0.5 # Retourner neutre si non configuré\n",
    "    try:\n",
    "        # Recherche plus ciblée\n",
    "        query = f'({pair.split(\"/\")[0]} OR {pair.replace(\"/\",\"\")}) flair:Discussion'\n",
    "        submissions = reddit.subreddit(subreddit).search(query, limit=20, sort='relevance', time_filter='month')\n",
    "        scores = [s.score for s in submissions]\n",
    "        if not scores:\n",
    "             return 0.5 # Retourner une valeur neutre si pas de posts\n",
    "        positive_ratio = sum(1 for s in scores if s > 0) / len(scores)\n",
    "        return positive_ratio\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur Reddit pour {subreddit} / {pair}: {e}\")\n",
    "        return 0.5 # Retourner neutre en cas d'erreur API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des données de marché\n",
    "from Morningstar.utils.data_manager import ExchangeDataManager\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np # Importer numpy ici\n",
    "from datetime import datetime, timedelta # Importer datetime ici\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_data(pairs, timeframe='1h', start_date=None, end_date=None):\n",
    "    exchange = ExchangeDataManager(exchange_name=\"kucoin\") # Utiliser Kucoin\n",
    "    await exchange.load_markets_async()\n",
    "    all_data = []\n",
    "    tasks = []\n",
    "    for pair in pairs:\n",
    "        print(f\"Préparation du téléchargement {pair}...\")\n",
    "        tasks.append(exchange.load_data(pair, timeframe, start_date=start_date, end_date=end_date))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    await exchange.close()\n",
    "\n",
    "    successful_data = []\n",
    "    for i, result in enumerate(results):\n",
    "        pair = pairs[i]\n",
    "        if isinstance(result, pd.DataFrame) and not result.empty:\n",
    "            print(f\"Données {pair} téléchargées.\")\n",
    "            result['pair'] = pair\n",
    "            successful_data.append(result)\n",
    "        else:\n",
    "            print(f\"Échec ou données vides pour {pair}: {result}\")\n",
    "            \n",
    "    if successful_data:\n",
    "        return pd.concat(successful_data)\n",
    "    else:\n",
    "        # Ne pas lever d'erreur ici, retourner un DataFrame vide\n",
    "        print(\"AVERTISSEMENT: Aucune donnée téléchargée pour aucune paire.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "# Utiliser les variables globales pairs, start_date, end_date\n",
    "raw_data = pd.DataFrame() # Initialiser à vide\n",
    "try:\n",
    "    raw_data = asyncio.get_event_loop().run_until_complete(fetch_data(pairs, '1h', start_date, end_date))\n",
    "    if raw_data.empty:\n",
    "        print(\"Aucune donnée téléchargée - vérifiez les paires et les dates. Création de données synthétiques.\")\n",
    "    else:\n",
    "        print(f\"Données brutes téléchargées : {raw_data.shape} lignes\")\n",
    "        print(raw_data.head())\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du téléchargement : {e}\")\n",
    "\n",
    "# Créer des données synthétiques si raw_data est vide\n",
    "if raw_data.empty:\n",
    "    print(\"Création d'un petit dataset synthétique pour continuer...\")\n",
    "    dates = pd.to_datetime([datetime.now() - timedelta(hours=i) for i in range(100)])\n",
    "    synth_data = {\n",
    "        'open': np.random.uniform(100, 200, 100),\n",
    "        'high': np.random.uniform(100, 210, 100),\n",
    "        'low': np.random.uniform(90, 200, 100),\n",
    "        'close': np.random.uniform(100, 200, 100),\n",
    "        'volume': np.random.uniform(1000, 5000, 100),\n",
    "        'pair': 'SYNTH/USDT'\n",
    "    }\n",
    "    raw_data = pd.DataFrame(synth_data)\n",
    "    raw_data['datetime'] = dates # Ajouter la colonne datetime\n",
    "    raw_data = raw_data.set_index('datetime') # Définir l'index si nécessaire\n",
    "    print(\"Dataset synthétique créé :\", raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fe008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement et sauvegarde\n",
    "from Morningstar.utils.custom_indicators import add_technical_indicators\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    if df.empty:\n",
    "        print(\"DataFrame d'entrée vide, impossible de préparer le dataset.\")\n",
    "        return df\n",
    "        \n",
    "    # S'assurer que l'index est DatetimeIndex si ce n'est pas déjà le cas\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        if 'datetime' in df.columns:\n",
    "             df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "             df = df.set_index('datetime')\n",
    "        else:\n",
    "             print(\"AVERTISSEMENT: Colonne 'datetime' manquante et index non DatetimeIndex.\")\n",
    "             # Tenter de réinitialiser si l'index n'est pas unique\n",
    "             if not df.index.is_unique:\n",
    "                  df = df.reset_index(drop=True)\n",
    "    elif not df.index.is_unique:\n",
    "         print(\"AVERTISSEMENT: L'index Datetime n'est pas unique, réinitialisation.\")\n",
    "         df = df.reset_index(drop=True) # Réinitialiser si DatetimeIndex mais non unique\n",
    "         \n",
    "    # Ajouter les indicateurs techniques\n",
    "    try:\n",
    "        df = add_technical_indicators(df.copy()) # Utiliser une copie\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'ajout des indicateurs: {e}. Indicateurs ignorés.\")\n",
    "    \n",
    "    # Initialiser les colonnes sociales\n",
    "    social_cols = ['commits', 'stars', 'forks', 'issues_opened', 'issues_closed', 'reddit_sentiment']\n",
    "    for col in social_cols:\n",
    "        df[col] = 0.0 # Initialiser à 0.0 pour type numérique\n",
    "        \n",
    "    # Récupérer les données sociales pour chaque paire si API configurée\n",
    "    repo_map = {\n",
    "        'BTC/USDT': 'bitcoin/bitcoin',\n",
    "        'ETH/USDT': 'ethereum/go-ethereum',\n",
    "        'BNB/USDT': 'binance-chain/docs',\n",
    "        'SOL/USDT': 'solana-labs/solana'\n",
    "    }\n",
    "    subreddit_map = {\n",
    "        'BTC/USDT': 'Bitcoin',\n",
    "        'ETH/USDT': 'ethereum',\n",
    "        'BNB/USDT': 'binance',\n",
    "        'SOL/USDT': 'solana'\n",
    "    }\n",
    "        \n",
    "    if 'pair' in df.columns:\n",
    "        for pair in df['pair'].unique():\n",
    "            mask = df['pair'] == pair\n",
    "            \n",
    "            if gh and pair in repo_map:\n",
    "                print(f\"Récupération GitHub pour {pair}...\")\n",
    "                commits, stars, forks, issues_opened, issues_closed = get_github_stats(repo_map[pair])\n",
    "                df.loc[mask, 'commits'] = commits\n",
    "                df.loc[mask, 'stars'] = stars\n",
    "                df.loc[mask, 'forks'] = forks\n",
    "                df.loc[mask, 'issues_opened'] = issues_opened\n",
    "                df.loc[mask, 'issues_closed'] = issues_closed\n",
    "                \n",
    "            if reddit and pair in subreddit_map:\n",
    "                print(f\"Récupération Reddit pour {pair}...\")\n",
    "                sentiment = get_reddit_sentiment(subreddit_map[pair], pair.split('/')[0])\n",
    "                df.loc[mask, 'reddit_sentiment'] = sentiment\n",
    "    else:\n",
    "        print(\"Colonne 'pair' manquante, impossible de récupérer les données sociales.\")\n",
    "    \n",
    "    # S'assurer que 'datetime' est une colonne pour la sélection finale\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "         df = df.reset_index() # Convertir l'index datetime en colonne\n",
    "         \n",
    "    final_columns = [\n",
    "        'open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'macd_signal', 'macd_hist',\n",
    "        'bb_upper', 'bb_middle', 'bb_lower', 'volume_ma', 'volume_anomaly', 'pair',\n",
    "        'commits', 'stars', 'forks', 'issues_opened', 'issues_closed', 'reddit_sentiment', 'datetime'\n",
    "    ]\n",
    "    \n",
    "    # S'assurer que toutes les colonnes finales existent et remplir les NaN\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0 # Initialiser à 0.0\n",
    "            \n",
    "    # Convertir les colonnes sociales en numérique et remplir NaN\n",
    "    for col in social_cols:\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)\n",
    "         \n",
    "    # Remplir les NaN restants (indicateurs techniques au début)\n",
    "    df = df.fillna(method='ffill').fillna(0.0)\n",
    "            \n",
    "    return df[final_columns] # Retourner avec l'ordre final\n",
    "\n",
    "data = prepare_dataset(raw_data.copy()) # Utiliser une copie\n",
    "\n",
    "if not data.empty:\n",
    "    data.to_parquet('full_dataset.parquet')\n",
    "    print(f\"Dataset final préparé: {data.shape}\")\n",
    "    print(data.info())\n",
    "    print(data.head())\n",
    "else:\n",
    "    print(\"Le dataset est vide après préparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04662a",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Morningstar\n",
    "import tensorflow as tf\n",
    "from Morningstar.workflows.training_workflow import TrainingWorkflow\n",
    "import numpy as np\n",
    "\n",
    "if data.empty:\n",
    "    print(\"Dataset vide, impossible d'entraîner le modèle.\")\n",
    "else:\n",
    "    print(f\"\\n=== Préparation des données pour TF ===\")\n",
    "    print(f\"Taille du dataset Pandas: {len(data)} échantillons\")\n",
    "\n",
    "    # Configuration adaptative\n",
    "    class ColabConfig:\n",
    "        def __init__(self):\n",
    "            self.time_window = min(50, max(10, len(data)//10)) \n",
    "            # Exclure les colonnes non numériques ou non pertinentes\n",
    "            self.features = [col for col in data.columns if data[col].dtype in [np.float64, np.int64] and col not in ['datetime', 'pair', 'close']] \n",
    "            # S'assurer que 'close' n'est pas dans les features si c'est la cible\n",
    "            # self.features = [col for col in data.columns if col not in ['datetime', 'pair', 'close']] \n",
    "            self.target_col = 'close' # Définir explicitement la cible\n",
    "            self.epochs = 200\n",
    "            self.batch_size = min(1024, max(32, len(data)//5))\n",
    "            self.dataset_path = 'full_dataset.parquet'\n",
    "\n",
    "    colab_config = ColabConfig()\n",
    "    print(f\"Configuration utilisée: {vars(colab_config)}\")\n",
    "\n",
    "    # Conversion en dataset TensorFlow avec vérification\n",
    "    print(\"\\n=== Conversion en dataset TensorFlow ===\")\n",
    "    workflow = TrainingWorkflow(colab_config)\n",
    "    try:\n",
    "        # Assurer que _prepare_dataset utilise self.features et self.target_col\n",
    "        tf_dataset = workflow._prepare_dataset(data) \n",
    "        \n",
    "        dataset_size = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "        print(f\"Taille du dataset TensorFlow: {dataset_size} éléments (avant batch)\")\n",
    "        \n",
    "        if dataset_size < colab_config.time_window + 1:\n",
    "             raise ValueError(f\"Dataset trop petit ({dataset_size}) pour la fenêtre temporelle ({colab_config.time_window})\")\n",
    "             \n",
    "        # Batching après vérification de taille\n",
    "        tf_dataset = tf_dataset.batch(colab_config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        batched_size = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "        print(f\"Taille du dataset TensorFlow après batching: {batched_size} batches\")\n",
    "        if batched_size < 1:\n",
    "             raise ValueError(\"Le dataset TensorFlow est vide après batching\")\n",
    "             \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors de la conversion TF: {str(e)}\")\n",
    "        # Optionnel: arrêter ou continuer avec un modèle non entraîné\n",
    "        raise e # Arrêter l'exécution\n",
    "\n",
    "    # Division train/val\n",
    "    print(\"\\n=== Division train/validation ===\")\n",
    "    # Diviser AVANT le batching est plus simple si _prepare_dataset retourne un dataset non batché\n",
    "    # Si _prepare_dataset retourne déjà batché, il faut unbatch/rebatch ou skip/take sur les batches\n",
    "    val_batches = max(1, int(batched_size * 0.2))\n",
    "    train_dataset = tf_dataset.skip(val_batches)\n",
    "    val_dataset = tf_dataset.take(val_batches)\n",
    "\n",
    "    print(f\"Train batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "    print(f\"Val batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "    \n",
    "    # Vérifier que les datasets ne sont pas vides après division\n",
    "    if tf.data.experimental.cardinality(train_dataset).numpy() == 0:\n",
    "        raise ValueError(\"Le dataset d'entraînement est vide après division.\")\n",
    "    if tf.data.experimental.cardinality(val_dataset).numpy() == 0:\n",
    "        print(\"Attention: Le dataset de validation est vide après division. Entraînement sans validation.\")\n",
    "        val_dataset = None\n",
    "\n",
    "    # Construction du modèle\n",
    "    print(\"\\n=== Construction du modèle ===\")\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        input_shape = (colab_config.time_window, len(colab_config.features))\n",
    "        print(f\"Input shape du modèle: {input_shape}\")\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = tf.keras.layers.Conv1D(128, 5, activation='swish')(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "        x = tf.keras.layers.LSTM(128)(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='swish')(x)\n",
    "        outputs = tf.keras.layers.Dense(1)(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='huber',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss' if val_dataset else 'loss'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss' if val_dataset else 'loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss' if val_dataset else 'loss', restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    # Entraînement\n",
    "    print(\"\\n=== Début de l'entraînement ===\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=colab_config.epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 # Afficher la progression\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de2f39",
   "metadata": {},
   "source": [
    "## 5. Sauvegarde finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde finale et export sur Google Drive\n",
    "if 'model' in locals(): # Vérifier si le modèle a été entraîné\n",
    "    model.save('morningstar_pro.h5')\n",
    "    print(\"Modèle Keras sauvegardé localement.\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        !mkdir -p '/content/drive/MyDrive/Colab Data/' # Créer le dossier si besoin\n",
    "        !cp morningstar_pro.h5 '/content/drive/MyDrive/Colab Data/'\n",
    "        print(\"Modèle copié sur Google Drive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la copie sur Google Drive: {e}\")\n",
    "else:\n",
    "    print(\"Aucun modèle n'a été entraîné (dataset vide ou erreur précédente).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
