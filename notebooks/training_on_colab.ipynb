{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morningstar Pro - Entraînement avancé sur Colab\n",
    "\n",
    "## Système complet de trading algorithmique avec données sociales\n",
    "\n",
    "Ce notebook permet :\n",
    "- De télécharger les données de marché (OHLCV) depuis un exchange crypto\n",
    "- D'ajouter des indicateurs techniques avancés\n",
    "- D'intégrer des données sociales (GitHub et Reddit)\n",
    "- D'entraîner un modèle de deep learning pour le trading"
   ]
  },
  {
   "cell_type": "markdown", 
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances système et Python\n",
    "!pip install -q tensorflow==2.12.0 pandas==1.5.3 numpy==1.23.5 ccxt==4.1.91 ta pyarrow scikit-learn asyncpraw tweepy aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70924d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonage du dépôt et ajout du chemin Morningstar\n",
    "!git clone https://github.com/Cabrel10/eva001.git\n",
    "import sys\n",
    "sys.path.insert(0, '/content/eva001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f64f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Morningstar Pro - Entraînement avancé sur Colab\\n\",\n",
    "    \"\\n\",\n",
    "    \"Ce notebook permet de choisir dynamiquement les paires et l’intervalle de dates pour télécharger les données, puis d’entraîner le modèle Morningstar sur Colab.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Installation des dépendances système et Python\\n\",\n",
    "    \"!pip install -q tensorflow==2.12.0 pandas==1.5.3 numpy==1.23.5 ccxt==4.1.91 ta pyarrow scikit-learn asyncpraw tweepy aiohttp PyGithub praw\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Clonage du dépôt et ajout du chemin Morningstar\\n\",\n",
    "    \"!git clone https://github.com/Cabrel10/eva001.git\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.insert(0, '/content/eva001')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Sélection interactive des paires et des dates\\n\",\n",
    "    \"import datetime\\n\",\n",
    "    \"default_pairs = 'BTC/USDT,ETH/USDT,BNB/USDT,SOL/USDT'\\n\",\n",
    "    \"pairs = input(f\\\"Entrez les paires séparées par une virgule (exemple: {default_pairs}): \\\") or default_pairs\\n\",\n",
    "    \"pairs = [p.strip() for p in pairs.split(',')]\\n\",\n",
    "    \"start_date = input(\\\"Date de début (YYYY-MM-DD, défaut 2023-01-01): \\\") or '2023-01-01'\\n\",\n",
    "    \"end_date = input(\\\"Date de fin (YYYY-MM-DD, défaut aujourd'hui): \\\") or str(datetime.date.today())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configuration des APIs sociales\\n\",\n",
    "    \"from github import Github\\n\",\n",
    "    \"import praw\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Config GitHub (remplacer par ton token)\\n\",\n",
    "    \"github_token = input(\\\"Entrez votre token GitHub (ou laissez vide pour désactiver): \\\") or None\\n\",\n",
    "    \"gh = Github(github_token) if github_token else None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Config Reddit (remplacer par tes credentials)\\n\",\n",
    "    \"reddit = praw.Reddit(\\n\",\n",
    "    \"    client_id=input(\\\"Reddit client_id: \\\") or None,\\n\",\n",
    "    \"    client_secret=input(\\\"Reddit client_secret: \\\") or None,\\n\",\n",
    "    \"    user_agent=\\\"Morningstar Data Collector\\\"\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Fonctions pour données sociales\\n\",\n",
    "    \"def get_github_stats(repo_name):\\n\",\n",
    "    \"    if not gh:\\n\",\n",
    "    \"        return None, None, None, None, None\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        repo = gh.get_repo(repo_name)\\n\",\n",
    "    \"        return (\\n\",\n",
    "    \"            repo.get_commits().totalCount,\\n\",\n",
    "    \"            repo.stargazers_count,\\n\",\n",
    "    \"            repo.forks_count,\\n\",\n",
    "    \"            repo.get_issues(state='open').totalCount,\\n\",\n",
    "    \"            repo.get_issues(state='closed').totalCount\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    except:\\n\",\n",
    "    \"        return None, None, None, None, None\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_reddit_sentiment(subreddit, pair):\\n\",\n",
    "    \"    if not reddit:\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        submissions = reddit.subreddit(subreddit).search(f\\\"{pair} flair:Discussion\\\", limit=10)\\n\",\n",
    "    \"        return sum(1 for s in submissions if s.score > 0) / 10  # Ratio de posts positifs\\n\",\n",
    "    \"    except:\\n\",\n",
    "    \"        return None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Pipeline de données avancé\\n\",\n",
    "    \"from Morningstar.utils.data_manager import ExchangeDataManager\\n\",\n",
    "    \"from Morningstar.utils.custom_indicators import add_technical_indicators\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import asyncio\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Correction pour l'event loop Colab/Jupyter\\n\",\n",
    "    \"!pip install nest_asyncio\\n\",\n",
    "    \"import nest_asyncio\\n\",\n",
    "    \"nest_asyncio.apply()\\n\",\n",
    "    \"\\n\",\n",
    "    \"async def fetch_data(pairs, timeframe='1d', start_date=None, end_date=None):\\n\",\n",
    "    \"    exchange = ExchangeDataManager(exchange_name=\\\"kucoin\\\")\\n\",\n",
    "    \"    await exchange.load_markets_async()\\n\",\n",
    "    \"    all_data = []\\n\",\n",
    "    \"    for pair in pairs:\\n\",\n",
    "    \"        print(f\\\"Téléchargement {pair}...\\\")\\n\",\n",
    "    \"        df = await exchange.load_data(pair, timeframe, start_date, end_date)\\n\",\n",
    "    \"        if not df.empty:\\n\",\n",
    "    \"            df['pair'] = pair\\n\",\n",
    "    \"            all_data.append(df)\\n\",\n",
    "    \"    await exchange.close()\\n\",\n",
    "    \"    if all_data:\\n\",\n",
    "    \"        return pd.concat(all_data)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise ValueError(\\\"Aucune donnée téléchargée.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"raw_data = asyncio.get_event_loop().run_until_complete(fetch_data(pairs, '1h', start_date, end_date))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Prétraitement et sauvegarde\\n\",\n",
    "    \"def prepare_dataset(df):\\n\",\n",
    "    \"    # Réinitialiser l'index pour éviter les doublons\\n\",\n",
    "    \"    df = df.reset_index(drop=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ajouter les indicateurs techniques\\n\",\n",
    "    \"    df = add_technical_indicators(df)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Récupérer les données sociales pour chaque paire\\n\",\n",
    "    \"    for pair in df['pair'].unique():\\n\",\n",
    "    \"        # Exemple: mapper BTC/USDT à un repo GitHub et subreddit\\n\",\n",
    "    \"        repo_map = {\\n\",\n",
    "    \"            'BTC/USDT': 'bitcoin/bitcoin',\\n\",\n",
    "    \"            'ETH/USDT': 'ethereum/go-ethereum',\\n\",\n",
    "    \"            'BNB/USDT': 'binance-chain/docs',\\n\",\n",
    "    \"            'SOL/USDT': 'solana-labs/solana'\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        subreddit_map = {\\n\",\n",
    "    \"            'BTC/USDT': 'Bitcoin',\\n\",\n",
    "    \"            'ETH/USDT': 'ethereum',\\n\",\n",
    "    \"            'BNB/USDT': 'binance',\\n\",\n",
    "    \"            'SOL/USDT': 'solana'\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if pair in repo_map:\\n\",\n",
    "    \"            commits, stars, forks, issues_opened, issues_closed = get_github_stats(repo_map[pair])\\n\",\n",
    "    \"            mask = df['pair'] == pair\\n\",\n",
    "    \"            df.loc[mask, 'commits'] = commits\\n\",\n",
    "    \"            df.loc[mask, 'stars'] = stars\\n\",\n",
    "    \"            df.loc[mask, 'forks'] = forks\\n\",\n",
    "    \"            df.loc[mask, 'issues_opened'] = issues_opened\\n\",\n",
    "    \"            df.loc[mask, 'issues_closed'] = issues_closed\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        if pair in subreddit_map:\\n\",\n",
    "    \"            sentiment = get_reddit_sentiment(subreddit_map[pair], pair.split('/')[0])\\n\",\n",
    "    \"            df.loc[df['pair'] == pair, 'reddit_sentiment'] = sentiment\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Colonnes finales\\n\",\n",
    "    \"    columns = [\\n\",\n",
    "    \"        'open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'macd_signal', 'macd_hist',\\n\",\n",
    "    \"        'bb_upper', 'bb_middle', 'bb_lower', 'volume_ma', 'volume_anomaly', 'pair',\\n\",\n",
    "    \"        'commits', 'stars', 'forks', 'issues_opened', 'issues_closed', 'reddit_sentiment', 'datetime'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # S'assurer que toutes les colonnes existent\\n\",\n",
    "    \"    for col in columns:\\n\",\n",
    "    \"        if col not in df.columns:\\n\",\n",
    "    \"            df[col] = None\\n\",\n",
    "    \"            \\n\",\n",
    "    \"    return df[columns]\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = prepare_dataset(raw_data)\\n\",\n",
    "    \"data.to_parquet('full_dataset.parquet')\\n\",\n",
    "    \"print(f\\\"Dataset final: {data.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Entraînement du modèle Morningstar\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from Morningstar.workflows.training_workflow import TrainingWorkflow\\n\",\n",
    "    \"class ColabConfig:\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        self.time_window = 50\\n\",\n",
    "    \"        self.features = data.columns.tolist()\\n\",\n",
    "    \"        self.epochs = 200\\n\",\n",
    "    \"        self.batch_size = 1024\\n\",\n",
    "    \"        self.dataset_path = 'full_dataset.parquet'\\n\",\n",
    "    \"colab_config = ColabConfig()\\n\",\n",
    "    \"workflow = TrainingWorkflow(colab_config)\\n\",\n",
    "    \"tf_dataset = workflow._prepare_dataset(data)\\n\",\n",
    "    \"dataset_size = tf.data.experimental.cardinality(tf_dataset).numpy()\\n\",\n",
    "    \"val_size = int(dataset_size * 0.2)\\n\",\n",
    "    \"train_dataset = tf_dataset.skip(val_size)\\n\",\n",
    "    \"val_dataset = tf_dataset.take(val_size)\\n\",\n",
    "    \"with tf.distribute.MirroredStrategy().scope():\\n\",\n",
    "    \"    inputs = tf.keras.Input(shape=(50, len(data.columns)))\\n\",\n",
    "    \"    x = tf.keras.layers.Conv1D(128, 5, activation='swish')(inputs)\\n\",\n",
    "    \"    x = tf.keras.layers.BatchNormalization()(x)\\n\",\n",
    "    \"    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\\n\",\n",
    "    \"    x = tf.keras.layers.LSTM(128)(x)\\n\",\n",
    "    \"    x = tf.keras.layers.Dense(64, activation='swish')(x)\\n\",\n",
    "    \"    outputs = tf.keras.layers.Dense(1)(x)\\n\",\n",
    "    \"    model = tf.keras.Model(inputs, outputs)\\n\",\n",
    "    \"    model.compile(\\n\",\n",
    "    \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",\n",
    "    \"        loss='huber',\\n\",\n",
    "    \"        metrics=['mae']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"callbacks = [\\n\",\n",
    "    \"    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\\n\",\n",
    "    \"    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\\n\",\n",
    "    \"    tf.keras.callbacks.TensorBoard(log_dir='./logs')\\n\",\n",
    "    \"]\\n\",\n",
    "    \"history = model.fit(\\n,\n",
    "    \"    train_dataset,\\n\",\n",
    "    \"    validation_data=val_dataset,\\n\",\n",
    "    \"    epochs=colab_config.epochs,\\n\",\n",
    "    \"    batch_size=colab_config.batch_size,\\n\",\n",
    "    \"    callbacks=callbacks\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Sauvegarde finale et export sur Google Drive\\n\",\n",
    "    \"model.save('morningstar_pro.h5')\\n\",\n",
    "    \"from google.colab import drive\\n\",\n",
    "    \"drive.mount('/content/drive')\\n\",\n",
    "    \"!cp morningstar_pro.h5 '/content/drive/MyDrive/Colab Data/'\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de données avancé\n",
    "from Morningstar.utils.data_manager import ExchangeDataManager\n",
    "from Morningstar.utils.custom_indicators import add_technical_indicators\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "\n",
    "async def fetch_data(pairs, timeframe='1d', start_date=None, end_date=None):\n",
    "    exchange = ExchangeDataManager(exchange_name=\"binance\")\n",
    "    await exchange.load_markets_async()\n",
    "    all_data = []\n",
    "    for pair in pairs:\n",
    "        print(f\"Téléchargement {pair}...\")\n",
    "        df = await exchange.load_data(pair, timeframe, start_date, end_date)\n",
    "        if not df.empty:\n",
    "            df['pair'] = pair\n",
    "            all_data.append(df)\n",
    "    await exchange.close()\n",
    "    if all_data:\n",
    "        return pd.concat(all_data)\n",
    "    else:\n",
    "        raise ValueError(\"Aucune donnée téléchargée.\")\n",
    "\n",
    "raw_data = asyncio.run(fetch_data(pairs, '1h', start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement et sauvegarde\n",
    "def prepare_dataset(df):\n",
    "    df = add_technical_indicators(df)\n",
    "    columns = [\n",
    "        'open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'macd_signal', 'macd_hist',\n",
    "        'bb_upper', 'bb_middle', 'bb_lower', 'volume_ma', 'volume_anomaly', 'pair',\n",
    "        'commits', 'stars', 'forks', 'issues_opened', 'issues_closed', 'datetime'\n",
    "    ]\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "data = prepare_dataset(raw_data)\n",
    "data.to_parquet('full_dataset.parquet')\n",
    "print(f\"Dataset final: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Morningstar\n",
    "import tensorflow as tf\n",
    "from Morningstar.workflows.training_workflow import TrainingWorkflow\n",
    "class ColabConfig:\n",
    "    def __init__(self):\n",
    "        self.time_window = 50\n",
    "        self.features = data.columns.tolist()\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 1024\n",
    "        self.dataset_path = 'full_dataset.parquet'\n",
    "colab_config = ColabConfig()\n",
    "workflow = TrainingWorkflow(colab_config)\n",
    "tf_dataset = workflow._prepare_dataset(data)\n",
    "dataset_size = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "val_size = int(dataset_size * 0.2)\n",
    "train_dataset = tf_dataset.skip(val_size)\n",
    "val_dataset = tf_dataset.take(val_size)\n",
    "with tf.distribute.MirroredStrategy().scope():\n",
    "    inputs = tf.keras.Input(shape=(50, len(data.columns)))\n",
    "    x = tf.keras.layers.Conv1D(128, 5, activation='swish')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(128)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='swish')(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=colab_config.epochs,\n",
    "    batch_size=colab_config.batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde finale et export sur Google Drive\n",
    "model.save('morningstar_pro.h5')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp morningstar_pro.h5 '/content/drive/MyDrive/Colab Data/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
