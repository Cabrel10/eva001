{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du Modèle Morningstar sur Google Colab\n",
    "\n",
    "Ce notebook guide à travers les étapes nécessaires pour entraîner le modèle hybride multi-tâches Morningstar dans l'environnement Google Colab.\n",
    "\n",
    "**Étapes principales :**\n",
    "1. Configuration de l'environnement (clonage du repo, installation des dépendances).\n",
    "2. Chargement et préparation des données pour un actif spécifique.\n",
    "3. Définition et compilation de l'architecture du modèle Morningstar.\n",
    "4. Entraînement du modèle avec suivi des performances.\n",
    "5. Évaluation du modèle entraîné.\n",
    "6. Visualisation des courbes d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'Environnement\n",
    "\n",
    "Nous devons d'abord cloner le dépôt contenant le code du projet et installer les dépendances nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloner le dépôt (remplacez par l'URL de votre dépôt)\n",
    "# Assurez-vous que le dépôt est accessible (public ou via token/clé SSH si privé)\n",
    "!git clone https://github.com/VOTRE_USER/VOTRE_REPO.git CryptoRobot\n",
    "\n",
    "# Se déplacer dans le répertoire du projet\n",
    "%cd CryptoRobot/Morningstar\n",
    "\n",
    "# Installer les dépendances\n",
    "# Assurez-vous que requirements.txt est à jour\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** Si vos données ne sont pas dans le dépôt, vous devrez les rendre accessibles à Colab, par exemple en :\n",
    "*   Montant votre Google Drive : `from google.colab import drive; drive.mount('/content/drive')` et en ajustant les chemins.\n",
    "*   Téléchargeant les fichiers Parquet directement dans l'environnement Colab (ex: via `wget` ou l'interface Colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Ajouter le répertoire racine du projet au PYTHONPATH pour les imports locaux\n",
    "# Ajustez si la structure clonée est différente\n",
    "sys.path.append('/content/CryptoRobot/Morningstar') \n",
    "\n",
    "from model.training.data_loader import load_and_split_data\n",
    "\n",
    "# --- Configuration --- \n",
    "ASSET_NAME = 'btc' # Choisir l'actif (btc, eth, sol, etc.)\n",
    "# Ajuster le chemin si les données ne sont pas dans le repo cloné\n",
    "DATA_DIR = Path('data/processed') \n",
    "FILE_PATH = DATA_DIR / f\"{ASSET_NAME}_final.parquet\"\n",
    "LABEL_COLUMNS = ['trading_signal', 'volatility', 'market_regime']\n",
    "VALIDATION_SPLIT = 0.2 # % des données pour la validation (fin de série)\n",
    "\n",
    "print(f\"Chargement des données pour : {ASSET_NAME} depuis {FILE_PATH}\")\n",
    "\n",
    "# Charger les données en tant que Tensors\n",
    "try:\n",
    "    X, y_dict = load_and_split_data(FILE_PATH, label_columns=LABEL_COLUMNS, as_tensor=True)\n",
    "    print(f\"Données chargées : X shape={X.shape}, Labels={list(y_dict.keys())}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR : Le fichier {FILE_PATH} n'a pas été trouvé. Vérifiez le chemin et la disponibilité des données.\")\n",
    "except ValueError as e:\n",
    "    print(f\"ERREUR lors du chargement/split : {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur inattendue est survenue : {e}\")\n",
    "\n",
    "# Séparation Train/Validation (temporelle)\n",
    "num_samples = X.shape[0]\n",
    "num_val_samples = int(num_samples * VALIDATION_SPLIT)\n",
    "num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "X_train, X_val = X[:num_train_samples], X[num_train_samples:]\n",
    "y_train_dict = {name: tensor[:num_train_samples] for name, tensor in y_dict.items()}\n",
    "y_val_dict = {name: tensor[num_train_samples:] for name, tensor in y_dict.items()}\n",
    "\n",
    "print(f\"Séparation Train/Validation : Train={num_train_samples}, Val={num_val_samples}\")\n",
    "print(f\"Shapes : X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "print(f\"Labels Train : {[f'{k}:{v.shape}' for k, v in y_train_dict.items()]}\")\n",
    "print(f\"Labels Val : {[f'{k}:{v.shape}' for k, v in y_val_dict.items()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition et Compilation du Modèle Morningstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.architecture.enhanced_hybrid_model import build_enhanced_hybrid_model\n",
    "\n",
    "# --- Configuration du Modèle --- \n",
    "INPUT_SHAPE = (X_train.shape[1],) \n",
    "# Doit correspondre à la cardinalité des labels de classification\n",
    "NUM_TRADING_CLASSES = 5 # Ex: Strong Sell -> Strong Buy\n",
    "NUM_REGIME_CLASSES = 3  # Ex: Bull, Bear, Sideways\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"Construction du modèle Morningstar...\")\n",
    "model = build_enhanced_hybrid_model(input_shape=INPUT_SHAPE, \n",
    "                                    num_trading_classes=NUM_TRADING_CLASSES, \n",
    "                                    num_regime_classes=NUM_REGIME_CLASSES)\n",
    "\n",
    "model.summary() # Afficher l'architecture\n",
    "\n",
    "# Définir les pertes et métriques (doivent correspondre aux noms des couches de sortie)\n",
    "losses = {\n",
    "    'trading_signal_output': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    'volatility_output': tf.keras.losses.MeanSquaredError(),\n",
    "    'market_regime_output': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "}\n",
    "metrics = {\n",
    "    'trading_signal_output': ['accuracy'],\n",
    "    'volatility_output': [tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'],\n",
    "    'market_regime_output': ['accuracy']\n",
    "}\n",
    "# loss_weights = {'trading_signal_output': 1.0, 'volatility_output': 0.5, 'market_regime_output': 0.8} # Optionnel\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "print(\"Compilation du modèle...\")\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=losses, \n",
    "              metrics=metrics)\n",
    "              # loss_weights=loss_weights)\n",
    "\n",
    "print(\"Modèle compilé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration de l'Entraînement ---\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "MODEL_SAVE_DIR = Path('model/training') # Chemin relatif dans le projet cloné\n",
    "MODEL_SAVE_PATH = MODEL_SAVE_DIR / f'{ASSET_NAME}_morningstar_colab.h5'\n",
    "\n",
    "# Créer le répertoire de sauvegarde si nécessaire\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_SAVE_PATH,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss', # Surveiller la perte totale de validation\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, # Nb epochs sans amélioration avant arrêt\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(f\"Début de l'entraînement pour {EPOCHS} epochs...\")\n",
    "print(f\"Le meilleur modèle sera sauvegardé dans : {MODEL_SAVE_PATH}\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_dict,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val_dict),\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Entraînement terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation du Modèle\n",
    "\n",
    "Évaluons les performances du modèle (avec les poids restaurés du meilleur epoch grâce à `restore_best_weights=True` dans EarlyStopping) sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Évaluation du meilleur modèle sur l'ensemble de validation...\")\n",
    "results = model.evaluate(X_val, y_val_dict, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "print(\"Résultats de l'évaluation:\")\n",
    "results_dict = {}\n",
    "try:\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        results_dict[name] = value\n",
    "        print(f\"  - {name}: {value:.4f}\")\n",
    "except AttributeError:\n",
    "    print(\"Impossible de récupérer model.metrics_names, affichage brut:\", results)\n",
    "\n",
    "# Optionnel : Charger explicitement le meilleur modèle sauvegardé si EarlyStopping n'a pas restauré les poids\n",
    "# print(f\"\\nChargement du meilleur modèle depuis {MODEL_SAVE_PATH} pour vérification...\")\n",
    "# best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "# results_best = best_model.evaluate(X_val, y_val_dict, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Résultats du modèle chargé:\")\n",
    "# try:\n",
    "#     for name, value in zip(best_model.metrics_names, results_best):\n",
    "#         print(f\"  - {name}: {value:.4f}\")\n",
    "# except AttributeError:\n",
    "#     print(\"Impossible de récupérer model.metrics_names, affichage brut:\", results_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des Courbes d'Apprentissage\n",
    "\n",
    "Visualisons l'évolution des pertes et des métriques clés pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "# Clés disponibles dans l'historique\n",
    "print(\"\\nClés disponibles dans l'historique:\", history_dict.keys())\n",
    "\n",
    "# Créer les graphiques\n",
    "epochs_range = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Perte Totale\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(epochs_range, history_dict['loss'], label='Train Loss')\n",
    "plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 2. Trading Signal Accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epochs_range, history_dict['trading_signal_output_accuracy'], label='Train Accuracy')\n",
    "plt.plot(epochs_range, history_dict['val_trading_signal_output_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Trading Signal Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 3. Volatility RMSE\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(epochs_range, history_dict['volatility_output_rmse'], label='Train RMSE')\n",
    "plt.plot(epochs_range, history_dict['val_volatility_output_rmse'], label='Validation RMSE')\n",
    "plt.title('Volatility RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Market Regime Accuracy\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(epochs_range, history_dict['market_regime_output_accuracy'], label='Train Accuracy')\n",
    "plt.plot(epochs_range, history_dict['val_market_regime_output_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Market Regime Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 5. Volatility MAE (si intéressante)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(epochs_range, history_dict['volatility_output_mae'], label='Train MAE')\n",
    "plt.plot(epochs_range, history_dict['val_volatility_output_mae'], label='Validation MAE')\n",
    "plt.title('Volatility MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du Notebook\n",
    "\n",
    "Le modèle a été entraîné, évalué, et le meilleur modèle a été sauvegardé. Les courbes d'apprentissage donnent un aperçu de la convergence et des éventuels problèmes (sur-apprentissage, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
