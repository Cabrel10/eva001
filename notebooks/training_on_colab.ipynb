{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du Modèle Morningstar sur Google Colab\n",
    "\n",
    "Ce notebook guide à travers les étapes nécessaires pour entraîner le modèle hybride multi-tâches Morningstar dans l'environnement Google Colab.\n",
    "\n",
    "**Pré-requis :** Assurez-vous que votre projet local (contenant ce notebook et le reste du code) est synchronisé sur votre Google Drive, par exemple dans `MyDrive/CryptoRobot/Morningstar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Monter Google Drive et Définir le Chemin du Projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- IMPORTANT : Adaptez ce chemin si nécessaire --- \n",
    "PROJECT_ROOT = Path('/content/drive/MyDrive/CryptoRobot/Morningstar')\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Le répertoire du projet n'a pas été trouvé à : {PROJECT_ROOT}. Vérifiez le chemin.\")\n",
    "\n",
    "# Se déplacer dans le répertoire du projet\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Répertoire courant : {os.getcwd()}\")\n",
    "\n",
    "# Ajouter le répertoire racine au PYTHONPATH pour les imports locaux\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    print(f\"Ajout de {PROJECT_ROOT} au sys.path\")\n",
    "    sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installation des Dépendances et Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Installation des dépendances ---\")\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"\\n--- Vérification GPU ---\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible : {gpu_devices}\")\n",
    "else:\n",
    "    print(\"Aucun GPU détecté. L'entraînement sera sur CPU (plus lent).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exécution du Pipeline de Données (Optionnel)\n",
    "\n",
    "Si les données traitées (`data/processed/*.parquet`) ne sont pas déjà sur votre Drive, vous pouvez exécuter le pipeline ici. Sinon, cette étape peut être sautée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommentez pour exécuter le pipeline si nécessaire\n",
    "# print(\"\\n--- Lancement du pipeline complet de données ---\")\n",
    "# !python tests/manual_tests/test_full_pipeline_all_assets.py\n",
    "# print(\"\\n--- Pipeline de données terminé ---\")\n",
    "# !ls -l data/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importer les modules locaux (le PYTHONPATH a été configuré précédemment)\n",
    "from model.training.data_loader import load_and_split_data\n",
    "from model.architecture.morningstar_model import MorningstarModel\n",
    "\n",
    "# --- Configuration --- \n",
    "ASSET_NAME = 'sol' # Choisir l'actif (btc, eth, sol, etc.)\n",
    "DATA_DIR = Path('data/processed') # Chemin relatif à PROJECT_ROOT\n",
    "FILE_PATH = DATA_DIR / f\"{ASSET_NAME}_final.parquet\"\n",
    "# --- IMPORTANT : Ces labels doivent correspondre aux sorties du modèle --- \n",
    "LABEL_COLUMNS = ['signal', 'volatility_quantiles', 'volatility_regime', 'market_regime', 'sl_tp']\n",
    "VALIDATION_SPLIT = 0.2 # % des données pour la validation (fin de série)\n",
    "\n",
    "print(f\"Chargement des données pour : {ASSET_NAME} depuis {FILE_PATH}\")\n",
    "\n",
    "# Charger les données en tant que Tensors\n",
    "X_technical = None\n",
    "X_llm = None\n",
    "y_dict = None\n",
    "if FILE_PATH.exists():\n",
    "    try:\n",
    "        (X_technical, X_llm), y_dict = load_and_split_data(\n",
    "            FILE_PATH, \n",
    "            label_columns=LABEL_COLUMNS, \n",
    "            as_tensor=True\n",
    "        )\n",
    "        print(f\"Données chargées : X_technical shape={X_technical.shape}, X_llm shape={X_llm.shape}, Labels={list(y_dict.keys())}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERREUR lors du chargement/split : {e}\")\n",
    "        print(\"Vérifiez que les LABEL_COLUMNS ci-dessus correspondent aux colonnes dans le fichier Parquet et que les embeddings LLM (simulés ou réels) sont présents.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue lors du chargement : {e}\")\n",
    "else:\n",
    "     print(f\"ERREUR : Le fichier {FILE_PATH} n'a pas été trouvé. Vérifiez le chemin et/ou exécutez le pipeline de données.\")\n",
    "\n",
    "# Continuer seulement si les données ont été chargées correctement\n",
    "if X_technical is not None and X_llm is not None and y_dict is not None:\n",
    "    # Séparation Train/Validation (temporelle)\n",
    "    num_samples = X_technical.shape[0]\n",
    "    num_val_samples = int(num_samples * VALIDATION_SPLIT)\n",
    "    num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "    # Split pour les features\n",
    "    X_technical_train = X_technical[:num_train_samples]\n",
    "    X_technical_val = X_technical[num_train_samples:]\n",
    "    \n",
    "    X_llm_train = X_llm[:num_train_samples]\n",
    "    X_llm_val = X_llm[num_train_samples:]\n",
    "\n",
    "    # Split pour les labels\n",
    "    y_train = {name: tensor[:num_train_samples] for name, tensor in y_dict.items()}\n",
    "    y_val = {name: tensor[num_train_samples:] for name, tensor in y_dict.items()}\n",
    "\n",
    "    # Préparation des dictionnaires d'inputs pour le modèle\n",
    "    X_train = {'technical_input': X_technical_train, 'llm_input': X_llm_train}\n",
    "    X_val = {'technical_input': X_technical_val, 'llm_input': X_llm_val}\n",
    "\n",
    "    print(f\"Séparation Train/Validation : Train={num_train_samples}, Val={num_val_samples}\")\n",
    "    print(f\"Shapes : X_technical_train={X_technical_train.shape}, X_llm_train={X_llm_train.shape}\")\n",
    "    print(f\"Labels Train : {[f'{k}:{v.shape}' for k, v in y_train.items()]}\")\n",
    "else:\n",
    "    print(\"\\nArrêt prématuré car les données n'ont pas pu être chargées correctement.\")\n",
    "    # Assigner des valeurs vides pour éviter les erreurs dans les cellules suivantes si on les exécute quand même\n",
    "    X_train, X_val, y_train, y_val = None, None, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialisation et Compilation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train is not None:\n",
    "    model_wrapper = MorningstarModel()\n",
    "    model_wrapper.initialize_model()\n",
    "    model = model_wrapper.model # Accéder au modèle Keras sous-jacent\n",
    "    \n",
    "    # --- IMPORTANT : Ces clés doivent correspondre aux noms des sorties du modèle --- \n",
    "    losses = {\n",
    "        'signal': 'sparse_categorical_crossentropy',\n",
    "        'volatility_quantiles': 'mse',\n",
    "        'volatility_regime': 'sparse_categorical_crossentropy',\n",
    "        'market_regime': 'sparse_categorical_crossentropy',\n",
    "        'sl_tp': 'mse'\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "        'signal': ['accuracy'],\n",
    "        'volatility_quantiles': ['mae'],\n",
    "        'volatility_regime': ['accuracy'],\n",
    "        'market_regime': ['accuracy'],\n",
    "        'sl_tp': ['mae']\n",
    "    }\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=losses,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Modèle initialisé et compilé avec succès.\")\n",
    "    print(model_wrapper.get_model_summary()) # Afficher le résumé via le wrapper\n",
    "else:\n",
    "    print(\"Définition/Compilation du modèle sautée car les données n'ont pas été chargées.\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_train is not None:\n",
    "    # --- Configuration de l'Entraînement ---\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    # Le modèle sera sauvegardé dans le répertoire du projet sur Drive\n",
    "    MODEL_SAVE_DIR = Path('model/training') \n",
    "    MODEL_SAVE_PATH = MODEL_SAVE_DIR / f'{ASSET_NAME}_morningstar_colab.h5'\n",
    "\n",
    "    # Créer le répertoire de sauvegarde si nécessaire\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_SAVE_PATH,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss', # Surveiller la perte totale de validation\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, # Nb epochs sans amélioration avant arrêt\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    print(f\"Début de l'entraînement pour {EPOCHS} epochs...\")\n",
    "    print(f\"Le meilleur modèle sera sauvegardé dans : {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, # Dictionnaire {'technical_input': ..., 'llm_input': ...}\n",
    "        y_train, # Dictionnaire de labels\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Entraînement terminé.\")\n",
    "else:\n",
    "    print(\"Entraînement sauté car les données ou le modèle ne sont pas prêts.\")\n",
    "    history = None # Pour éviter les erreurs suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_val is not None:\n",
    "    print(\"\\nÉvaluation du meilleur modèle sur l'ensemble de validation...\")\n",
    "    results = model.evaluate(X_val, y_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    print(\"Résultats de l'évaluation:\")\n",
    "    results_dict = {}\n",
    "    try:\n",
    "        for name, value in zip(model.metrics_names, results):\n",
    "            results_dict[name] = value\n",
    "            print(f\"  - {name}: {value:.4f}\")\n",
    "    except AttributeError:\n",
    "        print(\"Impossible de récupérer model.metrics_names, affichage brut:\", results)\n",
    "    \n",
    "    # Visualisation des courbes d'apprentissage\n",
    "    if history is not None:\n",
    "        import matplotlib.pyplot as plt\n",
    "        history_dict = history.history\n",
    "        print(\"\\nClés disponibles dans l'historique:\", history_dict.keys())\n",
    "\n",
    "        # Créer les graphiques\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            epochs_range = range(1, len(history_dict['loss']) + 1)\n",
    "            plt.figure(figsize=(15, 10))\n",
    "\n",
    "            # Perte Totale\n",
    "            plt.subplot(2, 3, 1)\n",
    "            plt.plot(epochs_range, history_dict['loss'], label='Train Loss')\n",
    "            plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')\n",
    "            plt.title('Total Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            # Métriques spécifiques (vérifier les noms exacts dans history_dict.keys())\n",
    "            metric_keys = [\n",
    "                ('signal_accuracy', 'Signal Accuracy'),\n",
    "                ('volatility_quantiles_mae', 'Volatility Quantiles MAE'),\n",
    "                ('volatility_regime_accuracy', 'Volatility Regime Accuracy'),\n",
    "                ('market_regime_accuracy', 'Market Regime Accuracy'),\n",
    "                ('sl_tp_mae', 'SL/TP MAE')\n",
    "            ]\n",
    "            \n",
    "            for i, (key_base, title) in enumerate(metric_keys, start=2):\n",
    "                train_key = key_base\n",
    "                val_key = f'val_{key_base}'\n",
    "                if train_key in history_dict and val_key in history_dict:\n",
    "                    plt.subplot(2, 3, i)\n",
    "                    plt.plot(epochs_range, history_dict[train_key], label=f'Train {title}')\n",
    "                    plt.plot(epochs_range, history_dict[val_key], label=f'Validation {title}')\n",
    "                    plt.title(title)\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.ylabel(title.split()[-1]) # Utilise le dernier mot comme label Y\n",
    "                    plt.legend()\n",
    "                else:\n",
    "                    print(f\"Métriques '{train_key}' ou '{val_key}' non trouvées dans l'historique.\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Données de perte non trouvées dans l'historique.\")\n",
    "else:\n",
    "    print(\"\\nÉvaluation et visualisation sautées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
