{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b547398",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Morningstar\n",
    "import tensorflow as tf\n",
    "from Morningstar.workflows.training_workflow import TrainingWorkflow\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\n=== Préparation des données ===\")\n",
    "print(f\"Taille du dataset original: {len(data)} échantillons\")\n",
    "print(f\"Colonnes: {data.columns.tolist()}\")\n",
    "\n",
    "# Vérification des données avant conversion\n",
    "print(\"\\nExemple de données avant conversion:\")\n",
    "print(data.iloc[0])\n",
    "\n",
    "# Configuration\n",
    "class ColabConfig:\n",
    "    def __init__(self):\n",
    "        self.time_window = 50\n",
    "        self.features = [col for col in data.columns if col not in ['datetime', 'pair']]\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 1024\n",
    "        self.dataset_path = 'full_dataset.parquet'\n",
    "\n",
    "colab_config = ColabConfig()\n",
    "print(f\"\\nConfiguration utilisée: {vars(colab_config)}\")\n",
    "\n",
    "# Conversion en dataset TensorFlow avec vérification\n",
    "print(\"\\n=== Conversion en dataset TensorFlow ===\")\n",
    "workflow = TrainingWorkflow(colab_config)\n",
    "try:\n",
    "    tf_dataset = workflow._prepare_dataset(data)\n",
    "    \n",
    "    # Vérification du dataset\n",
    "    print(\"\\nVérification du dataset TensorFlow:\")\n",
    "    sample = next(iter(tf_dataset))\n",
    "    print(f\"Type: {type(sample)}\")\n",
    "    print(f\"Contenu: {sample}\")\n",
    "    \n",
    "    dataset_size = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "    print(f\"Taille du dataset TensorFlow: {dataset_size} batches\")\n",
    "    \n",
    "    if dataset_size < 1:\n",
    "        raise ValueError(\"Le dataset TensorFlow est vide après conversion\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nERREUR lors de la conversion: {str(e)}\")\n",
    "    print(\"\\nDébogage - Tentative de conversion manuelle:\")\n",
    "    \n",
    "    # Conversion manuelle de secours\n",
    "    features = data[colab_config.features].values.astype(np.float32)\n",
    "    targets = data['close'].values.astype(np.float32)\n",
    "    \n",
    "    print(f\"Forme des features: {features.shape}\")\n",
    "    print(f\"Forme des targets: {targets.shape}\")\n",
    "    \n",
    "    # Création du dataset manuellement\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "    tf_dataset = tf_dataset.batch(colab_config.batch_size)\n",
    "    \n",
    "    print(\"\\nDataset créé manuellement avec succès\")\n",
    "    dataset_size = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "    print(f\"Taille du dataset manuel: {dataset_size} batches\")\n",
    "\n",
    "# Division train/val\n",
    "print(\"\\n=== Division train/validation ===\")\n",
    "val_size = max(1, int(dataset_size * 0.2))\n",
    "train_dataset = tf_dataset.skip(val_size)\n",
    "val_dataset = tf_dataset.take(val_size)\n",
    "\n",
    "print(f\"Train batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Val batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "\n",
    "# Construction du modèle\n",
    "print(\"\\n=== Construction du modèle ===\")\n",
    "with tf.distribute.MirroredStrategy().scope():\n",
    "    input_shape = (colab_config.time_window, len(colab_config.features))\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv1D(128, 5, activation='swish')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(128)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='swish')(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Entraînement\n",
    "print(\"\\n=== Début de l'entraînement ===\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=colab_config.epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a59ac",
   "metadata": {},
   "source": [
    "## 5. Sauvegarde finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b18936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde finale et export sur Google Drive\n",
    "model.save('morningstar_pro.h5')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp morningstar_pro.h5 '/content/drive/MyDrive/Colab Data/'\n",
    "print(\"Modèle sauvegardé avec succès\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
