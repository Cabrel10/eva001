{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section spécifique à Colab (commentée pour l'exécution locale) ---\n",
    "# !apt-get update && apt-get install -y tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du Modèle Morningstar sur Google Colab (Adapté pour Local)\n",
    "\n",
    "Ce notebook guide à travers les étapes nécessaires pour entraîner le modèle hybride multi-tâches Morningstar.\n",
    "Il a été adapté pour fonctionner dans l'environnement local tout en conservant les commandes spécifiques à Colab en commentaires.\n",
    "\n",
    "**Étapes principales :**\n",
    "1. Configuration de l'environnement (installation des dépendances, exécution du pipeline de données).\n",
    "2. Chargement et préparation des données pour un actif spécifique.\n",
    "3. Définition et compilation de l'architecture du modèle Morningstar.\n",
    "4. Entraînement du modèle avec suivi des performances.\n",
    "5. Évaluation du modèle entraîné.\n",
    "6. Visualisation des courbes d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'Environnement\n",
    "\n",
    "Nous devons d'abord installer les dépendances et générer les datasets via le pipeline.\n",
    "Les commandes spécifiques à Colab pour cloner le dépôt sont commentées ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clonage du Dépôt (Spécifique Colab - Commenté)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section spécifique à Colab (commentée pour l'exécution locale) ---\n",
    "# # Définir le répertoire de base pour le clonage\n",
    "# REPO_DIR = \"/content/CryptoRobot\"\n",
    "#\n",
    "# # Supprimer le répertoire s'il existe déjà pour un clonage propre\n",
    "# !rm -rf {REPO_DIR}\n",
    "#\n",
    "# # Cloner la branche spécifique 'mise-a-jour' du dépôt\n",
    "# # Assurez-vous que le dépôt est accessible (public ou via token/clé SSH si privé)\n",
    "# !git clone -b mise-a-jour https://github.com/Cabrel10/eva001.git {REPO_DIR}\n",
    "#\n",
    "# # Vérifier que le clonage a réussi\n",
    "# !ls -l /content\n",
    "# --- Fin de la section spécifique à Colab ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installation des Dépendances et Exécution du Pipeline\n",
    "\n",
    "Cette cellule configure l'environnement en utilisant des chemins absolus basés sur la racine du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section spécifique à Colab (commentée pour l'exécution locale) ---\n",
    "# # Se déplacer dans le répertoire cloné\n",
    "# %cd {REPO_DIR} \n",
    "# --- Fin de la section spécifique à Colab ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration Locale --- \n",
    "# Définir le chemin racine du projet\n",
    "PROJECT_ROOT = Path('/home/morningstar/Desktop/crypto_robot/Morningstar') # Ajustez si nécessaire\n",
    "\n",
    "# Vérifier le répertoire courant actuel (qui est 'notebooks/' au début de cette cellule)\n",
    "!pwd\n",
    "\n",
    "# Installer les dépendances depuis la racine du projet en utilisant le chemin absolu\n",
    "print(\"\\n--- Installation des dépendances ---\")\n",
    "requirements_path = PROJECT_ROOT / 'requirements.txt'\n",
    "# Utiliser une variable pour le chemin dans la commande shell (avec guillemets pour la robustesse)\n",
    "cmd_pip = f\"pip install -r '{requirements_path}'\"\n",
    "print(f\"Exécution: {cmd_pip}\")\n",
    "!{cmd_pip}\n",
    "\n",
    "# Exécuter le pipeline de données complet depuis la racine du projet en utilisant le chemin absolu\n",
    "print(\"\\n--- Lancement du pipeline complet de données ---\")\n",
    "pipeline_script_path = PROJECT_ROOT / 'tests' / 'manual_tests' / 'test_full_pipeline_all_assets.py'\n",
    "# Utiliser une variable pour le chemin dans la commande shell (avec guillemets)\n",
    "# Note: L'exécution de scripts Python avec '!' utilise le PYTHONPATH du kernel, \n",
    "# donc l'ajout de PROJECT_ROOT au sys.path dans la cellule suivante est toujours nécessaire pour les imports DANS le script.\n",
    "cmd_python = f\"python '{pipeline_script_path}'\"\n",
    "print(f\"Exécution: {cmd_python}\")\n",
    "!{cmd_python}\n",
    "\n",
    "print(\"\\n--- Pipeline de données terminé. Vérification des fichiers générés: ---\")\n",
    "# Vérifier le contenu du répertoire des données traitées en utilisant le chemin absolu\n",
    "processed_data_path = PROJECT_ROOT / 'data' / 'processed'\n",
    "# Utiliser une variable pour le chemin dans la commande shell (avec guillemets)\n",
    "cmd_ls = f\"ls -l '{processed_data_path}'\"\n",
    "print(f\"Exécution: {cmd_ls}\")\n",
    "!{cmd_ls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Préparation des Données\n",
    "\n",
    "Maintenant que les données sont générées dans `data/processed/` (relativement à la racine du projet), nous pouvons les charger.\n",
    "Nous devons ajouter le répertoire racine du projet au `sys.path` pour que Python trouve les modules locaux (ex: `model.training.data_loader`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Configuration Locale --- \n",
    "# Définir le chemin racine du projet\n",
    "PROJECT_ROOT = Path('/home/morningstar/Desktop/crypto_robot/Morningstar') # Ajustez si nécessaire\n",
    "\n",
    "# Vérifier si le répertoire courant est la racine du projet\n",
    "# Note: Le CWD peut avoir été modifié par %cd dans une cellule précédente si exécuté interactivement.\n",
    "current_cwd = Path.cwd()\n",
    "print(f\"Répertoire courant actuel : {current_cwd}\")\n",
    "if current_cwd != PROJECT_ROOT:\n",
    "    print(f\"Attention: Le répertoire courant n'est pas la racine du projet ({PROJECT_ROOT}).\")\n",
    "    # Tentative de changement vers la racine du projet si on est dans 'notebooks'\n",
    "    if current_cwd == PROJECT_ROOT / 'notebooks':\n",
    "        try:\n",
    "            print(f\"Tentative de changement de répertoire vers {PROJECT_ROOT}...\")\n",
    "            os.chdir(PROJECT_ROOT)\n",
    "            print(f\"Répertoire courant changé pour : {Path.cwd()}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERREUR: Impossible de trouver le répertoire projet {PROJECT_ROOT}\")\n",
    "    else:\n",
    "        print(\"Les chemins relatifs pourraient ne pas fonctionner comme prévu.\")\n",
    "\n",
    "# Ajouter le répertoire racine du projet au PYTHONPATH pour les imports locaux\n",
    "project_root_str = str(PROJECT_ROOT)\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Ajout de {project_root_str} au sys.path\")\n",
    "    sys.path.append(project_root_str)\n",
    "\n",
    "# --- Section spécifique à Colab (commentée) ---\n",
    "# # S'assurer que le répertoire courant est bien celui du projet\n",
    "# PROJECT_ROOT_COLAB = \"/content/CryptoRobot\" # Doit correspondre au REPO_DIR ci-dessus\n",
    "# if Path.cwd() != Path(PROJECT_ROOT_COLAB):\n",
    "#     print(f\"Changement du répertoire courant vers {PROJECT_ROOT_COLAB}\")\n",
    "#     os.chdir(PROJECT_ROOT_COLAB)\n",
    "# !pwd # Confirmer\n",
    "#\n",
    "# # Ajouter le répertoire racine du projet au PYTHONPATH\n",
    "# if PROJECT_ROOT_COLAB not in sys.path:\n",
    "#     print(f\"Ajout de {PROJECT_ROOT_COLAB} au sys.path\")\n",
    "#     sys.path.append(PROJECT_ROOT_COLAB)\n",
    "# --- Fin Section Colab ---\n",
    "\n",
    "# Importer le data_loader maintenant que le path est correct\n",
    "try:\n",
    "    from model.training.data_loader import load_and_split_data\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"ERREUR: Impossible d'importer 'model.training.data_loader'. Vérifiez:\")\n",
    "    print(f\"  - La structure du projet dans {PROJECT_ROOT}\")\n",
    "    print(f\"  - Que {PROJECT_ROOT} est bien dans sys.path: {sys.path}\")\n",
    "    print(f\"  - L'erreur originale: {e}\")\n",
    "    raise # Arrêter l'exécution\n",
    "\n",
    "# --- Configuration des Données --- \n",
    "ASSET_NAME = 'sol' # Choisir l'actif (btc, eth, sol, etc.)\n",
    "# Chemin relatif à la racine du projet (maintenant que CWD et sys.path sont corrects)\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'processed' \n",
    "FILE_PATH = DATA_DIR / f\"{ASSET_NAME}_final.parquet\"\n",
    "\n",
    "# --- IMPORTANT --- \n",
    "# Vérifiez que les colonnes générées par votre pipeline correspondent à celles-ci.\n",
    "# Si l'erreur persiste, ajustez cette liste ou corrigez le pipeline.\n",
    "LABEL_COLUMNS = ['trading_signal', 'volatility', 'market_regime']\n",
    "VALIDATION_SPLIT = 0.2 # % des données pour la validation (fin de série)\n",
    "\n",
    "print(f\"Vérification de l'existence du fichier : {FILE_PATH}\")\n",
    "print(f\"Existe: {FILE_PATH.exists()}\")\n",
    "\n",
    "print(f\"Chargement des données pour : {ASSET_NAME} depuis {FILE_PATH}\")\n",
    "\n",
    "# Charger les données en tant que Tensors\n",
    "X = None\n",
    "y_dict = None\n",
    "if FILE_PATH.exists():\n",
    "    try:\n",
    "        X, y_dict = load_and_split_data(FILE_PATH, label_columns=LABEL_COLUMNS, as_tensor=True)\n",
    "        print(f\"Données chargées : X shape={X.shape}, Labels={list(y_dict.keys())}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERREUR lors du chargement/split : {e}\")\n",
    "        print(\"Vérifiez que les LABEL_COLUMNS ci-dessus correspondent aux colonnes dans le fichier Parquet.\")\n",
    "        # Pour inspecter les colonnes : df = pd.read_parquet(FILE_PATH); print(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue lors du chargement : {e}\")\n",
    "else:\n",
    "     print(f\"ERREUR : Le fichier {FILE_PATH} n'a pas été trouvé. Vérifiez que le pipeline l'a bien généré pour cet actif et que le chemin est correct.\")\n",
    "\n",
    "# Continuer seulement si les données ont été chargées correctement\n",
    "if X is not None and y_dict is not None:\n",
    "    # Séparation Train/Validation (temporelle)\n",
    "    num_samples = X.shape[0]\n",
    "    num_val_samples = int(num_samples * VALIDATION_SPLIT)\n",
    "    num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "    X_train, X_val = X[:num_train_samples], X[num_train_samples:]\n",
    "    y_train_dict = {name: tensor[:num_train_samples] for name, tensor in y_dict.items()}\n",
    "    y_val_dict = {name: tensor[num_train_samples:] for name, tensor in y_dict.items()}\n",
    "\n",
    "    print(f\"Séparation Train/Validation : Train={num_train_samples}, Val={num_val_samples}\")\n",
    "    print(f\"Shapes : X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "    print(f\"Labels Train : {[f'{k}:{v.shape}' for k, v in y_train_dict.items()]}\")\n",
    "    print(f\"Labels Val : {[f'{k}:{v.shape}' for k, v in y_val_dict.items()]}\")\n",
    "else:\n",
    "    print(\"\\nArrêt prématuré car les données n'ont pas pu être chargées correctement.\")\n",
    "    # Assigner des valeurs vides pour éviter les erreurs dans les cellules suivantes si on les exécute quand même\n",
    "    X_train, X_val, y_train_dict, y_val_dict = None, None, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition et Compilation du Modèle Morningstar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuer seulement si les données d'entraînement existent\n",
    "if X_train is not None:\n",
    "    # Importer ici pour s'assurer que sys.path est correct\n",
    "    try:\n",
    "        from model.architecture.enhanced_hybrid_model import build_enhanced_hybrid_model\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"ERREUR: Impossible d'importer 'model.architecture.enhanced_hybrid_model'. Vérifiez la structure.\")\n",
    "        raise\n",
    "\n",
    "    # --- Configuration du Modèle --- \n",
    "    INPUT_SHAPE = (X_train.shape[1],) \n",
    "    # Doit correspondre à la cardinalité des labels de classification\n",
    "    NUM_TRADING_CLASSES = 5 # Ex: Strong Sell -> Strong Buy\n",
    "    NUM_REGIME_CLASSES = 3  # Ex: Bull, Bear, Sideways\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    print(\"Construction du modèle Morningstar...\")\n",
    "    model = build_enhanced_hybrid_model(input_shape=INPUT_SHAPE, \n",
    "                                        num_trading_classes=NUM_TRADING_CLASSES, \n",
    "                                        num_regime_classes=NUM_REGIME_CLASSES)\n",
    "\n",
    "    model.summary() # Afficher l'architecture\n",
    "\n",
    "    # Définir les pertes et métriques (doivent correspondre aux noms des couches de sortie)\n",
    "    # Assurez-vous que ces noms correspondent à ceux définis dans build_enhanced_hybrid_model\n",
    "    losses = {\n",
    "        'trading_signal_output': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        'volatility_output': tf.keras.losses.MeanSquaredError(),\n",
    "        'market_regime_output': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    }\n",
    "    metrics = {\n",
    "        'trading_signal_output': ['accuracy'],\n",
    "        'volatility_output': [tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'],\n",
    "        'market_regime_output': ['accuracy']\n",
    "    }\n",
    "    # loss_weights = {'trading_signal_output': 1.0, 'volatility_output': 0.5, 'market_regime_output': 0.8} # Optionnel\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    print(\"Compilation du modèle...\")\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=losses, \n",
    "                  metrics=metrics)\n",
    "                  # loss_weights=loss_weights)\n",
    "\n",
    "    print(\"Modèle compilé.\")\n",
    "else:\n",
    "    print(\"Définition/Compilation du modèle sautée car les données n'ont pas été chargées.\")\n",
    "    model = None # Pour éviter les erreurs suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuer seulement si le modèle est défini et les données existent\n",
    "if model is not None and X_train is not None:\n",
    "    # --- Configuration de l'Entraînement ---\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    # Utiliser le même ASSET_NAME que pour le chargement\n",
    "    # Chemin relatif à la racine du projet\n",
    "    MODEL_SAVE_DIR = PROJECT_ROOT / 'model' / 'training' \n",
    "    # Nom de fichier différent pour l'entraînement local\n",
    "    MODEL_SAVE_PATH_LOCAL = MODEL_SAVE_DIR / f'{ASSET_NAME}_morningstar_local.h5'\n",
    "    # Chemin Colab original (commenté)\n",
    "    # MODEL_SAVE_PATH_COLAB = MODEL_SAVE_DIR / f'{ASSET_NAME}_morningstar_colab.h5'\n",
    "\n",
    "    # Créer le répertoire de sauvegarde si nécessaire\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_SAVE_PATH_LOCAL, # Utiliser le chemin local\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss', # Surveiller la perte totale de validation\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, # Nb epochs sans amélioration avant arrêt\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    print(f\"Début de l'entraînement pour {EPOCHS} epochs...\")\n",
    "    print(f\"Le meilleur modèle sera sauvegardé dans : {MODEL_SAVE_PATH_LOCAL}\")\n",
    "    # print(f\"(Chemin Colab original commenté : {MODEL_SAVE_PATH_COLAB})\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_dict,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val_dict),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Entraînement terminé.\")\n",
    "else:\n",
    "    print(\"Entraînement sauté car les données ou le modèle ne sont pas prêts.\")\n",
    "    history = None # Pour éviter les erreurs suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation du Modèle\n",
    "\n",
    "Évaluons les performances du modèle (avec les poids restaurés du meilleur epoch grâce à `restore_best_weights=True` dans EarlyStopping) sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuer seulement si le modèle a été entraîné et les données de validation existent\n",
    "if model is not None and X_val is not None:\n",
    "    print(\"Évaluation du meilleur modèle sur l'ensemble de validation...\")\n",
    "    results = model.evaluate(X_val, y_val_dict, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    print(\"Résultats de l'évaluation:\")\n",
    "    results_dict = {}\n",
    "    try:\n",
    "        for name, value in zip(model.metrics_names, results):\n",
    "            results_dict[name] = value\n",
    "            print(f\"  - {name}: {value:.4f}\")\n",
    "    except AttributeError:\n",
    "        print(\"Impossible de récupérer model.metrics_names, affichage brut:\", results)\n",
    "\n",
    "    # Optionnel : Charger explicitement le meilleur modèle sauvegardé si EarlyStopping n'a pas restauré les poids\n",
    "    # print(f\"\\nChargement du meilleur modèle depuis {MODEL_SAVE_PATH_LOCAL} pour vérification...\")\n",
    "    # try:\n",
    "    #    best_model = tf.keras.models.load_model(MODEL_SAVE_PATH_LOCAL)\n",
    "    #    results_best = best_model.evaluate(X_val, y_val_dict, batch_size=BATCH_SIZE, verbose=0)\n",
    "    #    print(\"Résultats du modèle chargé:\")\n",
    "    #    for name, value in zip(best_model.metrics_names, results_best):\n",
    "    #        print(f\"  - {name}: {value:.4f}\")\n",
    "    # except Exception as e:\n",
    "    #    print(f\"Erreur lors du chargement/évaluation du modèle sauvegardé: {e}\")\n",
    "    # except AttributeError:\n",
    "    #    print(\"Impossible de récupérer model.metrics_names, affichage brut:\", results_best)\n",
    "else:\n",
    "    print(\"Évaluation sautée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des Courbes d'Apprentissage\n",
    "\n",
    "Visualisons l'évolution des pertes et des métriques clés pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuer seulement si l'entraînement a eu lieu\n",
    "if history is not None:\n",
    "    history_dict = history.history\n",
    "\n",
    "    # Clés disponibles dans l'historique\n",
    "    print(\"\\nClés disponibles dans l'historique:\", list(history_dict.keys()))\n",
    "\n",
    "    # Créer les graphiques\n",
    "    # Vérifier si les clés existent avant de plotter pour éviter les erreurs\n",
    "    if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "        epochs_range = range(1, len(history_dict['loss']) + 1)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # 1. Perte Totale\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(epochs_range, history_dict['loss'], label='Train Loss')\n",
    "        plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')\n",
    "        plt.title('Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Trading Signal Accuracy (si disponible)\n",
    "        # Les noms peuvent varier légèrement selon la version de TF/Keras\n",
    "        ts_acc_key = next((k for k in history_dict if k.startswith('trading_signal_output') and k.endswith('accuracy')), None)\n",
    "        val_ts_acc_key = next((k for k in history_dict if k.startswith('val_trading_signal_output') and k.endswith('accuracy')), None)\n",
    "        if ts_acc_key and val_ts_acc_key:\n",
    "            plt.subplot(2, 3, 2)\n",
    "            plt.plot(epochs_range, history_dict[ts_acc_key], label='Train Accuracy')\n",
    "            plt.plot(epochs_range, history_dict[val_ts_acc_key], label='Validation Accuracy')\n",
    "            plt.title('Trading Signal Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        else:\n",
    "             print(f\"Métriques 'trading_signal_output_accuracy' non trouvées (cherché: {ts_acc_key}, {val_ts_acc_key}).\")\n",
    "\n",
    "        # 3. Volatility RMSE (si disponible)\n",
    "        vol_rmse_key = next((k for k in history_dict if k.startswith('volatility_output') and k.endswith('rmse')), None)\n",
    "        val_vol_rmse_key = next((k for k in history_dict if k.startswith('val_volatility_output') and k.endswith('rmse')), None)\n",
    "        if vol_rmse_key and val_vol_rmse_key:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            plt.plot(epochs_range, history_dict[vol_rmse_key], label='Train RMSE')\n",
    "            plt.plot(epochs_range, history_dict[val_vol_rmse_key], label='Validation RMSE')\n",
    "            plt.title('Volatility RMSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.legend()\n",
    "        else:\n",
    "             print(f\"Métriques 'volatility_output_rmse' non trouvées (cherché: {vol_rmse_key}, {val_vol_rmse_key}).\")\n",
    "\n",
    "        # 4. Market Regime Accuracy (si disponible)\n",
    "        mr_acc_key = next((k for k in history_dict if k.startswith('market_regime_output') and k.endswith('accuracy')), None)\n",
    "        val_mr_acc_key = next((k for k in history_dict if k.startswith('val_market_regime_output') and k.endswith('accuracy')), None)\n",
    "        if mr_acc_key and val_mr_acc_key:\n",
    "            plt.subplot(2, 3, 4)\n",
    "            plt.plot(epochs_range, history_dict[mr_acc_key], label='Train Accuracy')\n",
    "            plt.plot(epochs_range, history_dict[val_mr_acc_key], label='Validation Accuracy')\n",
    "            plt.title('Market Regime Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        else:\n",
    "             print(f\"Métriques 'market_regime_output_accuracy' non trouvées (cherché: {mr_acc_key}, {val_mr_acc_key}).\")\n",
    "\n",
    "        # 5. Volatility MAE (si disponible et intéressante)\n",
    "        vol_mae_key = next((k for k in history_dict if k.startswith('volatility_output') and k.endswith('mae')), None)\n",
    "        val_vol_mae_key = next((k for k in history_dict if k.startswith('val_volatility_output') and k.endswith('mae')), None)\n",
    "        if vol_mae_key and val_vol_mae_key:\n",
    "            plt.subplot(2, 3, 5)\n",
    "            plt.plot(epochs_range, history_dict[vol_mae_key], label='Train MAE')\n",
    "            plt.plot(epochs_range, history_dict[val_vol_mae_key], label='Validation MAE')\n",
    "            plt.title('Volatility MAE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.legend()\n",
    "        else:\n",
    "             print(f\"Métriques 'volatility_output_mae' non trouvées (cherché: {vol_mae_key}, {val_vol_mae_key}).\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Données de perte ('loss', 'val_loss') non trouvées dans l'historique, impossible de générer les graphiques.\")\n",
    "else:\n",
    "    print(\"Visualisation sautée car l'entraînement n'a pas eu lieu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du Notebook\n",
    "\n",
    "Le modèle a été entraîné (si les données étaient correctes), évalué, et le meilleur modèle a été sauvegardé localement. Les courbes d'apprentissage donnent un aperçu de la convergence et des éventuels problèmes (sur-apprentissage, etc.).\n",
    "\n",
    "Pour utiliser sur Colab, décommentez les sections spécifiques à Colab et ajustez les chemins si nécessaire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
